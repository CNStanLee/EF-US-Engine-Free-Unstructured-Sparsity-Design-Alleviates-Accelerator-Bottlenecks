{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a6c00153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU, QuantIdentity\n",
    "from brevitas.quant import Int8WeightPerTensorFixedPoint, Int8ActPerTensorFixedPoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5490ceab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/changhong/prj/finn/notebooks/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST/data\n",
      "Finn root directory: /home/changhong/prj/finn/notebooks\n",
      "Build directory: /home/changhong/prj/finn/notebooks/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST/build\n",
      "Model directory: /home/changhong/prj/finn/notebooks/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST/model\n"
     ]
    }
   ],
   "source": [
    "notebook_name = \"/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST\"\n",
    "finn_root = os.getcwd()\n",
    "build_dir = finn_root+ notebook_name +\"/build\"\n",
    "model_dir = finn_root+ notebook_name +\"/model\"\n",
    "data_dir = finn_root+ notebook_name +\"/data\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(build_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Finn root directory: {finn_root}\")\n",
    "print(f\"Build directory: {build_dir}\")\n",
    "print(f\"Model directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7d5cc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 50\n",
    "LR = 0.001\n",
    "BATACH_SIZE = 128\n",
    "RANDOM_SEED = 1998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fd463ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_sparsity(model):\n",
    "    param_details = []  # 存储各层详细信息\n",
    "    \n",
    "    # 收集所有参数信息\n",
    "    for name, module in model.named_modules():\n",
    "        if not name:  # 跳过根模块\n",
    "            continue\n",
    "            \n",
    "        layer_type = module.__class__.__name__\n",
    "        \n",
    "        for param_name, param in module.named_parameters(recurse=False):\n",
    "            param_count = param.numel()\n",
    "            non_zero_count = torch.count_nonzero(param).item()\n",
    "            sparsity = 100 * (1 - non_zero_count / param_count) if param_count > 0 else 0\n",
    "            param_details.append((f\"{name}.{param_name}\", layer_type, param_count, non_zero_count, sparsity))\n",
    "    \n",
    "    # 打印详细参数信息（对齐列）\n",
    "    if param_details:\n",
    "        # 计算每列最大宽度\n",
    "        max_path = max(len(str(d[0])) for d in param_details)\n",
    "        max_type = max(len(str(d[1])) for d in param_details)\n",
    "        max_count = max(len(f\"{d[2]:,}\") for d in param_details)  # 带千位分隔符\n",
    "        max_nonzero = max(len(f\"{d[3]:,}\") for d in param_details)\n",
    "        \n",
    "        # 打印表头\n",
    "        print(f\"{'Parameter Path':<{max_path}} | {'Layer Type':<{max_type}} | {'Param Count':>{max_count}} | {'Non-zero':>{max_nonzero}} | {'Sparsity (%)':>10}\")\n",
    "        print(\"-\" * (max_path + max_type + max_count + max_nonzero + 30))  # 动态分隔线长度\n",
    "        \n",
    "        # 打印每行数据\n",
    "        for detail in param_details:\n",
    "            print(f\"{detail[0]:<{max_path}} | {detail[1]:<{max_type}} | {detail[2]:>{max_count},} | {detail[3]:>{max_nonzero},} | {detail[4]:>10.2f}%\")\n",
    "    \n",
    "    # 计算并打印总统计\n",
    "    if param_details:\n",
    "        total_params = sum(d[2] for d in param_details)\n",
    "        total_non_zero = sum(d[3] for d in param_details)\n",
    "        total_sparsity = 100 * (1 - total_non_zero / total_params) if total_params > 0 else 0\n",
    "        \n",
    "        print(\"-\" * (max_path + max_type + max_count + max_nonzero + 30))\n",
    "        print(f\"{'TOTAL':<{max_path}} | {'-':<{max_type}} | {total_params:>{max_count},} | {total_non_zero:>{max_nonzero},} | {total_sparsity:>10.2f}%\")\n",
    "        compression_ratio = total_params / total_non_zero if total_non_zero > 0 else float('inf')\n",
    "        print(f\"\\nCompression Ratio (pruning only): {compression_ratio:.2f}x\")\n",
    "        \n",
    "        # 如果考虑8-bit量化（1字节/参数）与FP32（4字节/参数）的对比\n",
    "        effective_compression = 4 * compression_ratio  # 4 = 32/8\n",
    "        print(f\"Effective Compression (pruning + quantization): {effective_compression:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0050867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Training set: 50000 samples\n",
      "Validation set: 10000 samples\n",
      "Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "\n",
    "# Define data transformations\n",
    "# Normalize using MNIST mean (0.1307) and std deviation (0.3081)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load full MNIST training set (60,000 samples)\n",
    "full_train_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=True,       # Load training set (not test set)\n",
    "    download=True,    # Download if not exists\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Standard academic split (5:1:1 ratio)\n",
    "# Train: 50,000 | Val: 10,000 | Test: 10,000 (official test set)\n",
    "train_size = 50000    # 5/6 of training data\n",
    "val_size = 10000      # 1/6 of training data\n",
    "\n",
    "# Split the full training set into train/val subsets\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED)  # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Load official test set (10,000 samples)\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=False,      # Load test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATACH_SIZE,\n",
    "    shuffle=True      # Shuffle training data\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATACH_SIZE,\n",
    "    shuffle=False     # No need to shuffle validation data\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATACH_SIZE,  # Larger batch for evaluation\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"Dataset split complete:\")\n",
    "print(f\"Training set: {len(train_dataset)} samples\")\n",
    "print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "print(f\"Test set: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "503c7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.core.scaling import ScalingImplType\n",
    "import torch\n",
    "\n",
    "\n",
    "total_bits = 8   #width for weights and activations\n",
    "n = 7            #fractional part\n",
    "class LeNet5(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # Changhong, add input quant layer\n",
    "        # self.quant_input = qnn.QuantIdentity(\n",
    "        #     quant_type=QuantType.INT,\n",
    "        #     bit_width=total_bits,\n",
    "        #     max_val=1.0,  # 假设输入数据已归一化到 [0,1]\n",
    "        #     restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "        #     scaling_impl_type=ScalingImplType.CONST\n",
    "        # )\n",
    "        self.quant_input = qnn.QuantIdentity(\n",
    "            quant_type=QuantType.INT,\n",
    "            bit_width=8,\n",
    "            scaling_init=1.0,               # 初始缩放因子\n",
    "            scaling_impl_type=ScalingImplType.PARAMETER,  # 允许训练中调整\n",
    "            scaling_per_output_channel=False,  # 对于单通道输入，通常不需要每个输出通道的缩放\n",
    "            )\n",
    "\n",
    "\n",
    "        self.conv1 = qnn.QuantConv2d(in_channels= 1,\n",
    "                                     out_channels= 20,\n",
    "                                     kernel_size= 3,\n",
    "                                     padding= 1,\n",
    "                                     bias= False,\n",
    "                                     weight_quant_type=QuantType.INT, \n",
    "                                     weight_bit_width= total_bits,\n",
    "                                     weight_restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "                                     weight_scaling_impl_type=ScalingImplType.CONST,\n",
    "                                     weight_scaling_const=1.0)\n",
    "        self.relu1 = qnn.QuantReLU(quant_type=QuantType.INT, \n",
    "                                   bit_width=8, \n",
    "                                   max_val= 1- 1/128.0,\n",
    "                                   restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "                                   scaling_impl_type=ScalingImplType.CONST )\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "\n",
    "        self.conv2 = qnn.QuantConv2d(in_channels= 20,\n",
    "                                     out_channels= 50,\n",
    "                                     kernel_size= 3,\n",
    "                                     padding= 1,\n",
    "                                     bias= False,\n",
    "                                     weight_quant_type=QuantType.INT, \n",
    "                                     weight_bit_width=8,\n",
    "                                     weight_restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "                                     weight_scaling_impl_type=ScalingImplType.CONST,\n",
    "                                     weight_scaling_const=1.0 )\n",
    "\n",
    "        self.relu2 = qnn.QuantReLU(quant_type=QuantType.INT, \n",
    "                                   bit_width=8, \n",
    "                                   max_val= 1- 1/128.0,\n",
    "                                   restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "                                   scaling_impl_type=ScalingImplType.CONST )\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        \"\"\"\n",
    "        # for 32-bit precision FC layers\n",
    "        self.fc1   = nn.Linear(7*7*50, 500)\n",
    "\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2   = nn.Linear(500,10)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # for fixed-point precision FC layers\n",
    "        self.fc1   = qnn.QuantLinear(7*7*50, 500,\n",
    "                                     bias= True,\n",
    "                                     weight_quant_type=QuantType.INT, \n",
    "                                     weight_bit_width=32,\n",
    "                                     weight_restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "                                     weight_scaling_impl_type=ScalingImplType.CONST,\n",
    "                                     weight_scaling_const=1.0)\n",
    "        \n",
    "\n",
    "        self.relu3 = qnn.QuantReLU(quant_type=QuantType.INT, \n",
    "                                   bit_width=8, \n",
    "                                   max_val= 1- 1/128.0,\n",
    "                                   restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "                                   scaling_impl_type=ScalingImplType.CONST )\n",
    "\n",
    "        self.fc2   = qnn.QuantLinear(500, 10,\n",
    "                                     bias= True,\n",
    "                                     weight_quant_type=QuantType.INT, \n",
    "                                     weight_bit_width=8,\n",
    "                                     weight_restrict_scaling_type=RestrictValueType.POWER_OF_TWO,\n",
    "                                     weight_scaling_impl_type=ScalingImplType.CONST,\n",
    "                                     weight_scaling_const=1.0)\n",
    "    def forward(self, x):\n",
    "        out = self.quant_input(x)  # Apply input quantization\n",
    "        out = self.relu1(self.conv1(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.relu3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7b6c2251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0adc5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def test(model, test_loader, device):\n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # For multi-class classification (10 classes for MNIST)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # get the index of the max log-probability\n",
    "            \n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    print(f\"Test Top-1 accuracy(%): {accuracy_score(y_true, y_pred) * 100:.2f}%\")\n",
    "    print(f\"Test Top-1 error rate(%):) {(1 - accuracy_score(y_true, y_pred)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1838431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Path                                                                     | Layer Type       | Param Count |  Non-zero | Sparsity (%)\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "quant_input.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterScaling |         1 |         1 |       0.00%\n",
      "conv1.weight                                                                       | QuantConv2d      |       180 |       180 |       0.00%\n",
      "conv2.weight                                                                       | QuantConv2d      |     9,000 |     9,000 |       0.00%\n",
      "fc1.weight                                                                         | QuantLinear      | 1,225,000 | 1,225,000 |       0.00%\n",
      "fc1.bias                                                                           | QuantLinear      |       500 |       500 |       0.00%\n",
      "fc2.weight                                                                         | QuantLinear      |     5,000 |     5,000 |       0.00%\n",
      "fc2.bias                                                                           | QuantLinear      |        10 |        10 |       0.00%\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL                                                                              | -                | 1,239,691 | 1,239,691 |       0.00%\n",
      "\n",
      "Compression Ratio (pruning only): 1.00x\n",
      "Effective Compression (pruning + quantization): 4.00x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/changhong/prj/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Top-1 accuracy(%): 99.13%\n",
      "Test Top-1 error rate(%):) 0.87%\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "model.load_state_dict(torch.load(model_dir + \"/lenet_mnist_int8.pth\"))\n",
    "analyze_model_sparsity(model)\n",
    "model.to(device)\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e17f06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== After Pruning ==================================================\n",
      "Parameter Path                                                                     | Layer Type       | Param Count | Non-zero | Sparsity (%)\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "quant_input.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterScaling |         1 |     1 |       0.00%\n",
      "conv1.weight                                                                       | QuantConv2d      |       180 |   180 |       0.00%\n",
      "conv2.weight                                                                       | QuantConv2d      |     9,000 | 5,400 |      40.00%\n",
      "fc1.weight                                                                         | QuantLinear      | 1,225,000 | 6,125 |      99.50%\n",
      "fc1.bias                                                                           | QuantLinear      |       500 |   500 |       0.00%\n",
      "fc2.bias                                                                           | QuantLinear      |        10 |    10 |       0.00%\n",
      "fc2.weight                                                                         | QuantLinear      |     5,000 | 2,500 |      50.00%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL                                                                              | -                | 1,239,691 | 14,716 |      98.81%\n",
      "\n",
      "Compression Ratio (pruning only): 84.24x\n",
      "Effective Compression (pruning + quantization): 336.96x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/changhong/prj/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Top-1 accuracy(%): 64.38%\n",
      "Test Top-1 error rate(%):) 35.62%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "def apply_l1_pruning_type(model, layer_type, pruning_percentage=0.5):\n",
    "    # layer_type: QuantLinear or QuantConv2d\n",
    "    model_tmp = copy.deepcopy(model).to(device)\n",
    "    for name, module in model_tmp.named_modules():\n",
    "        if isinstance(module, layer_type):\n",
    "            # if the para number <100, skip pruning\n",
    "            if module.weight.numel() > 10:\n",
    "                prune.l1_unstructured(module, name='weight', amount=pruning_percentage)\n",
    "                prune.remove(module, 'weight')  \n",
    "    return model_tmp\n",
    "\n",
    "\n",
    "\n",
    "def apply_l1_pruning_by_param_path(model, param_path, pruning_percentage=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    改进版：确保能正确剪枝所有层类型\n",
    "    \n",
    "    参数:\n",
    "        verbose: 打印剪枝详细信息\n",
    "    \"\"\"\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    model_copy.to(device)\n",
    "    \n",
    "    # 分割路径\n",
    "    parts = param_path.split('.')\n",
    "    param_name = parts[-1]\n",
    "    module_path = '.'.join(parts[:-1])\n",
    "    \n",
    "    # 递归查找模块\n",
    "    target_module = model_copy\n",
    "    if module_path:  # 如果不是顶级参数\n",
    "        for part in module_path.split('.'):\n",
    "            try:\n",
    "                target_module = getattr(target_module, part)\n",
    "            except AttributeError:\n",
    "                raise ValueError(f\"Module path '{module_path}' not found (failed at '{part}')\")\n",
    "    \n",
    "    # 检查参数\n",
    "    if not hasattr(target_module, param_name):\n",
    "        raise ValueError(f\"Parameter '{param_name}' not found in module '{module_path}'\")\n",
    "    \n",
    "    param = getattr(target_module, param_name)\n",
    "    if verbose:\n",
    "        print(f\"Pruning {param_path} | Shape: {tuple(param.shape)} | Elements: {param.numel()}\")\n",
    "\n",
    "    # 执行剪枝（移除数量>1的限制）\n",
    "    prune.l1_unstructured(target_module, name=param_name, amount=pruning_percentage)\n",
    "    prune.remove(target_module, param_name)\n",
    "    \n",
    "    return model_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pmodel = apply_l1_pruning_by_param_path(model, 'conv1.weight', 0)\n",
    "pmodel = apply_l1_pruning_by_param_path(pmodel, 'conv2.weight', 0.4)\n",
    "pmodel = apply_l1_pruning_by_param_path(pmodel, 'fc1.weight', 0.995)\n",
    "pmodel = apply_l1_pruning_by_param_path(pmodel, 'fc1.bias', 0)\n",
    "pmodel = apply_l1_pruning_by_param_path(pmodel, 'fc2.weight', 0.5)\n",
    "pmodel = apply_l1_pruning_by_param_path(pmodel, 'fc2.weight', 0.5)\n",
    "\n",
    "print(\"=\"*50 + \" After Pruning \" + \"=\"*50)\n",
    "\n",
    "analyze_model_sparsity(pmodel) \n",
    "test(pmodel, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b1fd29dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/changhong/prj/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Loss: 0.1120 | Acc: 97.38%\n",
      "Epoch 2/30 | Loss: 0.0379 | Acc: 98.98%\n",
      "Epoch 3/30 | Loss: 0.0283 | Acc: 99.23%\n",
      "Epoch 4/30 | Loss: 0.0230 | Acc: 99.34%\n",
      "Epoch 5/30 | Loss: 0.0194 | Acc: 99.46%\n",
      "Epoch 6/30 | Loss: 0.0163 | Acc: 99.58%\n",
      "Epoch 7/30 | Loss: 0.0145 | Acc: 99.64%\n",
      "Epoch 8/30 | Loss: 0.0122 | Acc: 99.73%\n",
      "Epoch 9/30 | Loss: 0.0107 | Acc: 99.77%\n",
      "Epoch 10/30 | Loss: 0.0094 | Acc: 99.80%\n",
      "Epoch 11/30 | Loss: 0.0082 | Acc: 99.83%\n",
      "Epoch 12/30 | Loss: 0.0072 | Acc: 99.86%\n",
      "Epoch 13/30 | Loss: 0.0065 | Acc: 99.89%\n",
      "Epoch 14/30 | Loss: 0.0056 | Acc: 99.91%\n",
      "Epoch 15/30 | Loss: 0.0047 | Acc: 99.95%\n",
      "Epoch 16/30 | Loss: 0.0042 | Acc: 99.94%\n",
      "Epoch 17/30 | Loss: 0.0036 | Acc: 99.97%\n",
      "Epoch 18/30 | Loss: 0.0033 | Acc: 99.97%\n",
      "Epoch 19/30 | Loss: 0.0028 | Acc: 99.98%\n",
      "Epoch 20/30 | Loss: 0.0024 | Acc: 99.98%\n",
      "Epoch 21/30 | Loss: 0.0021 | Acc: 99.99%\n",
      "Epoch 22/30 | Loss: 0.0020 | Acc: 99.99%\n",
      "Epoch 23/30 | Loss: 0.0020 | Acc: 99.99%\n",
      "Epoch 24/30 | Loss: 0.0013 | Acc: 100.00%\n",
      "Epoch 25/30 | Loss: 0.0016 | Acc: 99.99%\n",
      "Epoch 26/30 | Loss: 0.0014 | Acc: 100.00%\n",
      "Epoch 27/30 | Loss: 0.0018 | Acc: 99.99%\n",
      "Epoch 28/30 | Loss: 0.0009 | Acc: 100.00%\n",
      "Epoch 29/30 | Loss: 0.0007 | Acc: 100.00%\n",
      "Epoch 30/30 | Loss: 0.0006 | Acc: 100.00%\n",
      "================================================== After Retraining ==================================================\n",
      "Parameter Path                                                                     | Layer Type       | Param Count | Non-zero | Sparsity (%)\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "quant_input.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value | ParameterScaling |         1 |     1 |       0.00%\n",
      "conv1.weight                                                                       | QuantConv2d      |       180 |   180 |       0.00%\n",
      "conv2.weight                                                                       | QuantConv2d      |     9,000 | 5,400 |      40.00%\n",
      "fc1.weight                                                                         | QuantLinear      | 1,225,000 | 6,125 |      99.50%\n",
      "fc1.bias                                                                           | QuantLinear      |       500 |   500 |       0.00%\n",
      "fc2.bias                                                                           | QuantLinear      |        10 |    10 |       0.00%\n",
      "fc2.weight                                                                         | QuantLinear      |     5,000 | 2,500 |      50.00%\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TOTAL                                                                              | -                | 1,239,691 | 14,716 |      98.81%\n",
      "\n",
      "Compression Ratio (pruning only): 84.24x\n",
      "Effective Compression (pruning + quantization): 336.96x\n",
      "Test Top-1 accuracy(%): 98.97%\n",
      "Test Top-1 error rate(%):) 1.03%\n"
     ]
    }
   ],
   "source": [
    "def freeze_zero_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            mask = (param != 0).float()\n",
    "            param.register_hook(lambda grad, mask=mask: grad * mask)\n",
    "\n",
    "def retrain_model(model, train_loader, num_epochs=5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()  # MNIST使用交叉熵损失\n",
    "    \n",
    "    # 冻结零权重（确保你的freeze_zero_weights实现支持稀疏梯度）\n",
    "    freeze_zero_weights(model)  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            inputs, target = data\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            # MNIST数据不需要.float()转换\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)  # 移除了.float()\n",
    "            loss = criterion(output, target)  # 注意target不需要.float()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 跳过冻结权重的更新\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and torch.all(param == 0):\n",
    "                    param.grad = None\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # 计算统计量\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "retrained_model = retrain_model(pmodel, train_loader, num_epochs=30)\n",
    "print(\"=\"*50 + \" After Retraining \" + \"=\"*50)\n",
    "analyze_model_sparsity(retrained_model)   # retrained model\n",
    "test(retrained_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "94b7b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changhong/prj/finn/deps/brevitas/src/brevitas/quant_tensor/int_quant_tensor.py:32: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  signed = torch.tensor(signed, dtype=torch.bool)\n",
      "/home/changhong/prj/finn/deps/brevitas/src/brevitas/quant_tensor/int_quant_tensor.py:34: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  training = torch.tensor(training, dtype=torch.bool)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/changhong/prj/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# accuracy drop down: 99.28% -> 99.01% => 0.27% < 0.3%\n",
    "# it is no-loss compression\n",
    "torch.save(retrained_model.state_dict(), model_dir +\"/\"+'lenet_minst_int8_prune' + \".pth\")\n",
    "\n",
    "# Export to ONNX\n",
    "retrained_model.cpu()  # Ensure model is on CPU for ONNX export\n",
    "onnx_model_path = model_dir + \"/lenet_mnist_int8_pruned.onnx\"\n",
    "torch.onnx.export(retrained_model,\n",
    "                  torch.randn(1, 1, 28, 28),\n",
    "                  onnx_model_path,\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "                  opset_version=11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "df5b8e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/changhong/prj/finn/notebooks/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST/model/lenet_mnist_int8_prune_ready.onnx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ready_model_filename = model_dir + \"/lenet_mnist_int8_prune_ready.onnx\"\n",
    "\n",
    "input_shape = (1, 1, 28, 28)\n",
    "\n",
    "input_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\n",
    "input_a = 2 * input_a - 1\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "#Move to CPU before export\n",
    "retrained_model.cpu()\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    retrained_model, export_path=ready_model_filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(ready_model_filename, out_file=ready_model_filename)\n",
    "\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ab14961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU, QuantIdentity\n",
    "from brevitas.quant import Int8WeightPerTensorFixedPoint, Int8ActPerTensorFixedPoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "88cc3a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 20 14:41:12 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 4000 Ada Gene...    Off |   00000000:02:00.0  On |                  Off |\n",
      "| 30%   41C    P8             11W /  130W |    3078MiB /  20475MiB |     32%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "48297cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/changhong/prj/finn/notebooks\n"
     ]
    }
   ],
   "source": [
    "# print current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5cc6c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/changhong/prj/finn/notebooks/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST_BNN/data\n",
      "Finn root directory: /home/changhong/prj/finn/notebooks\n",
      "Build directory: /home/changhong/prj/finn/notebooks/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST_BNN/build\n",
      "Model directory: /home/changhong/prj/finn/notebooks/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST_BNN/model\n"
     ]
    }
   ],
   "source": [
    "notebook_name = \"/EF-US-Engine-Free-Unstructured-Sparsity-Design-Alleviates-Accelerator-Bottlenecks/casestudy1/LeNet_MNIST_BNN\"\n",
    "finn_root = os.getcwd()\n",
    "build_dir = finn_root+ notebook_name +\"/build\"\n",
    "model_dir = finn_root+ notebook_name +\"/model\"\n",
    "data_dir = finn_root+ notebook_name +\"/data\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(build_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Finn root directory: {finn_root}\")\n",
    "print(f\"Build directory: {build_dir}\")\n",
    "print(f\"Model directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b47a20",
   "metadata": {},
   "source": [
    "# Model define\n",
    "This CNV model is modified from brevitas one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "09a4974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import BatchNorm1d\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.nn import QuantConv2d\n",
    "from brevitas.nn import QuantIdentity\n",
    "from brevitas.nn import QuantLinear\n",
    "\n",
    "from brevitas_examples.bnn_pynq.models.common import CommonActQuant\n",
    "from brevitas_examples.bnn_pynq.models.common import CommonWeightQuant\n",
    "from brevitas_examples.bnn_pynq.models.tensor_norm import TensorNorm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3d5a5",
   "metadata": {},
   "source": [
    "# Build a standard CNV 1W1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a922e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "CNV_OUT_CH_POOL = [(64, False), (64, True), (128, False), (128, True), (256, False), (256, False)]\n",
    "INTERMEDIATE_FC_FEATURES = [(256, 512), (512, 512)]\n",
    "LAST_FC_IN_FEATURES = 512\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "POOL_SIZE = 2\n",
    "KERNEL_SIZE = 3\n",
    "\n",
    "\n",
    "class CNV(Module):\n",
    "\n",
    "    def __init__(self, num_classes, weight_bit_width, act_bit_width, in_bit_width, in_ch):\n",
    "        super(CNV, self).__init__()\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "\n",
    "        self.conv_features.append(QuantIdentity( # for Q1.7 input format\n",
    "            act_quant=CommonActQuant,\n",
    "            bit_width=in_bit_width,\n",
    "            min_val=- 1.0,\n",
    "            max_val=1.0 - 2.0 ** (-7),\n",
    "            narrow_range=False,\n",
    "            restrict_scaling_type=RestrictValueType.POWER_OF_TWO))\n",
    "\n",
    "        for out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(\n",
    "                QuantConv2d(\n",
    "                    kernel_size=KERNEL_SIZE,\n",
    "                    in_channels=in_ch,\n",
    "                    out_channels=out_ch,\n",
    "                    bias=False,\n",
    "                    weight_quant=CommonWeightQuant,\n",
    "                    weight_bit_width=weight_bit_width))\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm2d(in_ch, eps=1e-4))\n",
    "            self.conv_features.append(\n",
    "                QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width))\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool2d(kernel_size=2))\n",
    "\n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(\n",
    "                QuantLinear(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    bias=False,\n",
    "                    weight_quant=CommonWeightQuant,\n",
    "                    weight_bit_width=weight_bit_width))\n",
    "            self.linear_features.append(BatchNorm1d(out_features, eps=1e-4))\n",
    "            self.linear_features.append(\n",
    "                QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width))\n",
    "\n",
    "        self.linear_features.append(\n",
    "            QuantLinear(\n",
    "                in_features=LAST_FC_IN_FEATURES,\n",
    "                out_features=num_classes,\n",
    "                bias=False,\n",
    "                weight_quant=CommonWeightQuant,\n",
    "                weight_bit_width=weight_bit_width))\n",
    "        self.linear_features.append(TensorNorm())\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuantConv2d) or isinstance(m, QuantLinear):\n",
    "                torch.nn.init.uniform_(m.weight.data, -1, 1)\n",
    "\n",
    "    def clip_weights(self, min_val, max_val):\n",
    "        for mod in self.conv_features:\n",
    "            if isinstance(mod, QuantConv2d):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "        for mod in self.linear_features:\n",
    "            if isinstance(mod, QuantLinear):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "def cnv(cfg):\n",
    "    weight_bit_width = cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH')\n",
    "    act_bit_width = cfg.getint('QUANT', 'ACT_BIT_WIDTH')\n",
    "    in_bit_width = cfg.getint('QUANT', 'IN_BIT_WIDTH')\n",
    "    num_classes = cfg.getint('MODEL', 'NUM_CLASSES')\n",
    "    in_channels = cfg.getint('MODEL', 'IN_CHANNELS')\n",
    "    net = CNV(\n",
    "        weight_bit_width=weight_bit_width,\n",
    "        act_bit_width=act_bit_width,\n",
    "        in_bit_width=in_bit_width,\n",
    "        num_classes=num_classes,\n",
    "        in_ch=in_channels)\n",
    "    return net\n",
    "\n",
    "cfg_6c3f_1w1a_minst = configparser.ConfigParser()\n",
    "cfg_6c3f_1w1a_minst['MODEL'] = {\n",
    "    'NUM_CLASSES': '10',\n",
    "    'IN_CHANNELS': '1',\n",
    "    'DTASET': 'MNIST',\n",
    "}\n",
    "cfg_6c3f_1w1a_minst['QUANT'] = {\n",
    "    'WEIGHT_BIT_WIDTH': '1',\n",
    "    'ACT_BIT_WIDTH': '1',\n",
    "    'IN_BIT_WIDTH': '8',\n",
    "}\n",
    "\n",
    "cfg_6c3f_1w1a_minst = cnv(cfg_6c3f_1w1a_minst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b2510640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model to ONNX format\n",
    "onnx_model_path = model_dir + \"/cfg_6c3f_1w1a_minst.onnx\"\n",
    "# torch.onnx.export(cfg_6c3f_1w1a_minst, \n",
    "#                   torch.randn(1, 1, 32, 32), \n",
    "#                   onnx_model_path, \n",
    "#                   input_names=['input'], \n",
    "#                   output_names=['output'], \n",
    "#                   dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n",
    "\n",
    "\n",
    "export_qonnx(cfg_6c3f_1w1a_minst, torch.randn(1, 1, 32, 32), onnx_model_path)\n",
    "qonnx_cleanup(onnx_model_path, out_file=onnx_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4360ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Training set: 48000 samples\n",
      "Validation set: 12000 samples\n",
      "Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "             transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "\n",
    "full_train_set = datasets.MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(full_train_set))\n",
    "val_size = len(full_train_set) - train_size\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(full_train_set, [train_size, val_size])\n",
    "test_set = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Dataset split complete:\")\n",
    "print(f\"Training set: {len(train_set)} samples\")\n",
    "print(f\"Validation set: {len(val_set)} samples\")\n",
    "print(f\"Test set: {len(test_set)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec4042a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = running_loss / len(loader)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c980615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/changhong/prj/finn/deps/brevitas/src/brevitas/nn/quant_linear.py:69: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:350.)\n",
      "  output_tensor = linear(x, quant_weight, quant_bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 1.7303, Train Acc: 41.40%, Val Loss: 1.4144, Val Acc: 54.18%\n",
      "Epoch [2/500], Train Loss: 1.1747, Train Acc: 61.98%, Val Loss: 1.1386, Val Acc: 62.98%\n",
      "Epoch [3/500], Train Loss: 0.8407, Train Acc: 72.95%, Val Loss: 0.8872, Val Acc: 70.85%\n",
      "Epoch [4/500], Train Loss: 0.6460, Train Acc: 78.91%, Val Loss: 0.6163, Val Acc: 80.15%\n",
      "Epoch [5/500], Train Loss: 0.5361, Train Acc: 82.32%, Val Loss: 1.2813, Val Acc: 57.26%\n",
      "Epoch [6/500], Train Loss: 0.4582, Train Acc: 84.88%, Val Loss: 0.7028, Val Acc: 76.43%\n",
      "Epoch [7/500], Train Loss: 0.4010, Train Acc: 86.61%, Val Loss: 1.7232, Val Acc: 47.83%\n",
      "Epoch [8/500], Train Loss: 0.3576, Train Acc: 88.13%, Val Loss: 0.5696, Val Acc: 81.11%\n",
      "Epoch [9/500], Train Loss: 0.3310, Train Acc: 89.30%, Val Loss: 0.8391, Val Acc: 73.70%\n",
      "Epoch [10/500], Train Loss: 0.3040, Train Acc: 90.13%, Val Loss: 1.6309, Val Acc: 50.98%\n",
      "Epoch [11/500], Train Loss: 0.2795, Train Acc: 91.07%, Val Loss: 0.4561, Val Acc: 85.47%\n",
      "Epoch [12/500], Train Loss: 0.2596, Train Acc: 91.65%, Val Loss: 0.8751, Val Acc: 72.12%\n",
      "Epoch [13/500], Train Loss: 0.2410, Train Acc: 92.39%, Val Loss: 0.6840, Val Acc: 79.32%\n",
      "Epoch [14/500], Train Loss: 0.2265, Train Acc: 92.95%, Val Loss: 0.3887, Val Acc: 88.19%\n",
      "Epoch [15/500], Train Loss: 0.2152, Train Acc: 93.12%, Val Loss: 2.3719, Val Acc: 45.60%\n",
      "Epoch [16/500], Train Loss: 0.2037, Train Acc: 93.49%, Val Loss: 0.5738, Val Acc: 82.69%\n",
      "Epoch [17/500], Train Loss: 0.1926, Train Acc: 93.97%, Val Loss: 0.2551, Val Acc: 91.90%\n",
      "Epoch [18/500], Train Loss: 0.1812, Train Acc: 94.30%, Val Loss: 0.1736, Val Acc: 94.67%\n",
      "Epoch [19/500], Train Loss: 0.1820, Train Acc: 94.24%, Val Loss: 1.1011, Val Acc: 66.98%\n",
      "Epoch [20/500], Train Loss: 0.1719, Train Acc: 94.56%, Val Loss: 0.3084, Val Acc: 90.06%\n",
      "Epoch [21/500], Train Loss: 0.1656, Train Acc: 94.79%, Val Loss: 1.9697, Val Acc: 45.39%\n",
      "Epoch [22/500], Train Loss: 0.1566, Train Acc: 95.17%, Val Loss: 2.4907, Val Acc: 37.12%\n",
      "Epoch [23/500], Train Loss: 0.1528, Train Acc: 95.19%, Val Loss: 2.5208, Val Acc: 43.88%\n",
      "Epoch [24/500], Train Loss: 0.1474, Train Acc: 95.40%, Val Loss: 2.7198, Val Acc: 35.67%\n",
      "Epoch [25/500], Train Loss: 0.1479, Train Acc: 95.46%, Val Loss: 1.0889, Val Acc: 68.15%\n",
      "Epoch [26/500], Train Loss: 0.1393, Train Acc: 95.77%, Val Loss: 0.9967, Val Acc: 68.33%\n",
      "Epoch [27/500], Train Loss: 0.1352, Train Acc: 95.74%, Val Loss: 0.4094, Val Acc: 87.56%\n",
      "Epoch [28/500], Train Loss: 0.1284, Train Acc: 95.95%, Val Loss: 0.2499, Val Acc: 92.48%\n",
      "Epoch [29/500], Train Loss: 0.1290, Train Acc: 95.98%, Val Loss: 2.7582, Val Acc: 34.12%\n",
      "Epoch [30/500], Train Loss: 0.1251, Train Acc: 96.11%, Val Loss: 0.1768, Val Acc: 94.47%\n",
      "Epoch [31/500], Train Loss: 0.1239, Train Acc: 96.18%, Val Loss: 0.2464, Val Acc: 92.33%\n",
      "Epoch [32/500], Train Loss: 0.1212, Train Acc: 96.14%, Val Loss: 0.9376, Val Acc: 73.62%\n",
      "Epoch [33/500], Train Loss: 0.1175, Train Acc: 96.31%, Val Loss: 1.2981, Val Acc: 60.48%\n",
      "Epoch [34/500], Train Loss: 0.1109, Train Acc: 96.59%, Val Loss: 0.8474, Val Acc: 73.78%\n",
      "Epoch [35/500], Train Loss: 0.1124, Train Acc: 96.50%, Val Loss: 1.7314, Val Acc: 53.15%\n",
      "Epoch [36/500], Train Loss: 0.1066, Train Acc: 96.70%, Val Loss: 2.8061, Val Acc: 33.10%\n",
      "Epoch [37/500], Train Loss: 0.1097, Train Acc: 96.59%, Val Loss: 0.6122, Val Acc: 81.57%\n",
      "Epoch [38/500], Train Loss: 0.1083, Train Acc: 96.68%, Val Loss: 0.1729, Val Acc: 94.66%\n",
      "Epoch [39/500], Train Loss: 0.1078, Train Acc: 96.55%, Val Loss: 1.5486, Val Acc: 54.97%\n",
      "Epoch [40/500], Train Loss: 0.1039, Train Acc: 96.79%, Val Loss: 2.4169, Val Acc: 39.75%\n",
      "Epoch [41/500], Train Loss: 0.1017, Train Acc: 96.76%, Val Loss: 0.1069, Val Acc: 96.61%\n",
      "Epoch [42/500], Train Loss: 0.0965, Train Acc: 96.95%, Val Loss: 0.1659, Val Acc: 95.03%\n",
      "Epoch [43/500], Train Loss: 0.1000, Train Acc: 96.85%, Val Loss: 0.2209, Val Acc: 93.19%\n",
      "Epoch [44/500], Train Loss: 0.0955, Train Acc: 96.96%, Val Loss: 1.5856, Val Acc: 59.79%\n",
      "Epoch [45/500], Train Loss: 0.0961, Train Acc: 97.04%, Val Loss: 3.9242, Val Acc: 25.81%\n",
      "Epoch [46/500], Train Loss: 0.0941, Train Acc: 97.03%, Val Loss: 3.2289, Val Acc: 33.52%\n",
      "Epoch [47/500], Train Loss: 0.0919, Train Acc: 97.09%, Val Loss: 2.6434, Val Acc: 41.17%\n",
      "Epoch [48/500], Train Loss: 0.0900, Train Acc: 97.17%, Val Loss: 2.5413, Val Acc: 41.14%\n",
      "Epoch [49/500], Train Loss: 0.0914, Train Acc: 97.16%, Val Loss: 0.3258, Val Acc: 89.91%\n",
      "Epoch [50/500], Train Loss: 0.0888, Train Acc: 97.26%, Val Loss: 1.0266, Val Acc: 68.49%\n",
      "Epoch [51/500], Train Loss: 0.0869, Train Acc: 97.29%, Val Loss: 1.7796, Val Acc: 52.52%\n",
      "Epoch [52/500], Train Loss: 0.0886, Train Acc: 97.19%, Val Loss: 0.2401, Val Acc: 92.69%\n",
      "Epoch [53/500], Train Loss: 0.0909, Train Acc: 97.10%, Val Loss: 0.3058, Val Acc: 89.93%\n",
      "Epoch [54/500], Train Loss: 0.0869, Train Acc: 97.27%, Val Loss: 0.2448, Val Acc: 92.88%\n",
      "Epoch [55/500], Train Loss: 0.0850, Train Acc: 97.34%, Val Loss: 1.3188, Val Acc: 63.07%\n",
      "Epoch [56/500], Train Loss: 0.0820, Train Acc: 97.35%, Val Loss: 1.2717, Val Acc: 61.62%\n",
      "Epoch [57/500], Train Loss: 0.0823, Train Acc: 97.40%, Val Loss: 1.9455, Val Acc: 53.06%\n",
      "Epoch [58/500], Train Loss: 0.0785, Train Acc: 97.56%, Val Loss: 0.1881, Val Acc: 93.92%\n",
      "Epoch [59/500], Train Loss: 0.0805, Train Acc: 97.51%, Val Loss: 0.2728, Val Acc: 91.89%\n",
      "Epoch [60/500], Train Loss: 0.0821, Train Acc: 97.44%, Val Loss: 0.5833, Val Acc: 82.21%\n",
      "Epoch [61/500], Train Loss: 0.0775, Train Acc: 97.59%, Val Loss: 0.2178, Val Acc: 92.87%\n",
      "Epoch [62/500], Train Loss: 0.0758, Train Acc: 97.69%, Val Loss: 0.2153, Val Acc: 93.37%\n",
      "Epoch [63/500], Train Loss: 0.0762, Train Acc: 97.67%, Val Loss: 0.2473, Val Acc: 92.24%\n",
      "Epoch [64/500], Train Loss: 0.0760, Train Acc: 97.63%, Val Loss: 1.6241, Val Acc: 55.80%\n",
      "Epoch [65/500], Train Loss: 0.0746, Train Acc: 97.67%, Val Loss: 0.8370, Val Acc: 76.53%\n",
      "Epoch [66/500], Train Loss: 0.0749, Train Acc: 97.56%, Val Loss: 0.1581, Val Acc: 95.09%\n",
      "Epoch [67/500], Train Loss: 0.0735, Train Acc: 97.68%, Val Loss: 0.0816, Val Acc: 97.68%\n",
      "Epoch [68/500], Train Loss: 0.0739, Train Acc: 97.71%, Val Loss: 0.1062, Val Acc: 96.75%\n",
      "Epoch [69/500], Train Loss: 0.0713, Train Acc: 97.77%, Val Loss: 0.0950, Val Acc: 97.09%\n",
      "Epoch [70/500], Train Loss: 0.0719, Train Acc: 97.80%, Val Loss: 0.8359, Val Acc: 72.08%\n",
      "Epoch [71/500], Train Loss: 0.0688, Train Acc: 97.83%, Val Loss: 2.3540, Val Acc: 39.97%\n",
      "Epoch [72/500], Train Loss: 0.0696, Train Acc: 97.79%, Val Loss: 0.0685, Val Acc: 97.84%\n",
      "Epoch [73/500], Train Loss: 0.0681, Train Acc: 97.81%, Val Loss: 0.3346, Val Acc: 89.22%\n",
      "Epoch [74/500], Train Loss: 0.0696, Train Acc: 97.83%, Val Loss: 0.1898, Val Acc: 94.10%\n",
      "Epoch [75/500], Train Loss: 0.0692, Train Acc: 97.86%, Val Loss: 1.6509, Val Acc: 53.77%\n",
      "Epoch [76/500], Train Loss: 0.0701, Train Acc: 97.81%, Val Loss: 0.2337, Val Acc: 92.45%\n",
      "Epoch [77/500], Train Loss: 0.0676, Train Acc: 97.88%, Val Loss: 0.2725, Val Acc: 91.43%\n",
      "Epoch [78/500], Train Loss: 0.0690, Train Acc: 97.84%, Val Loss: 2.8022, Val Acc: 45.26%\n",
      "Epoch [79/500], Train Loss: 0.0629, Train Acc: 98.04%, Val Loss: 0.1609, Val Acc: 95.27%\n",
      "Epoch [80/500], Train Loss: 0.0664, Train Acc: 97.89%, Val Loss: 0.1275, Val Acc: 96.09%\n",
      "Epoch [81/500], Train Loss: 0.0647, Train Acc: 97.94%, Val Loss: 2.0301, Val Acc: 51.35%\n",
      "Epoch [82/500], Train Loss: 0.0644, Train Acc: 98.00%, Val Loss: 0.2319, Val Acc: 92.63%\n",
      "Epoch [83/500], Train Loss: 0.0635, Train Acc: 97.98%, Val Loss: 0.6952, Val Acc: 79.08%\n",
      "Epoch [84/500], Train Loss: 0.0606, Train Acc: 98.04%, Val Loss: 0.3688, Val Acc: 88.92%\n",
      "Epoch [85/500], Train Loss: 0.0637, Train Acc: 98.00%, Val Loss: 0.6058, Val Acc: 80.27%\n",
      "Epoch [86/500], Train Loss: 0.0625, Train Acc: 98.06%, Val Loss: 0.2686, Val Acc: 91.54%\n",
      "Epoch [87/500], Train Loss: 0.0634, Train Acc: 98.00%, Val Loss: 0.2995, Val Acc: 91.04%\n",
      "Epoch [88/500], Train Loss: 0.0593, Train Acc: 98.05%, Val Loss: 0.2247, Val Acc: 93.17%\n",
      "Epoch [89/500], Train Loss: 0.0611, Train Acc: 98.04%, Val Loss: 1.1183, Val Acc: 67.31%\n",
      "Epoch [90/500], Train Loss: 0.0589, Train Acc: 98.11%, Val Loss: 0.1087, Val Acc: 96.75%\n",
      "Epoch [91/500], Train Loss: 0.0608, Train Acc: 98.01%, Val Loss: 0.9424, Val Acc: 70.82%\n",
      "Epoch [92/500], Train Loss: 0.0603, Train Acc: 98.10%, Val Loss: 0.9917, Val Acc: 69.67%\n",
      "Epoch [93/500], Train Loss: 0.0605, Train Acc: 98.06%, Val Loss: 0.1562, Val Acc: 95.22%\n",
      "Epoch [94/500], Train Loss: 0.0598, Train Acc: 98.03%, Val Loss: 1.6403, Val Acc: 59.59%\n",
      "Epoch [95/500], Train Loss: 0.0595, Train Acc: 98.14%, Val Loss: 0.0659, Val Acc: 97.92%\n",
      "Epoch [96/500], Train Loss: 0.0591, Train Acc: 98.12%, Val Loss: 2.4927, Val Acc: 50.45%\n",
      "Epoch [97/500], Train Loss: 0.0577, Train Acc: 98.17%, Val Loss: 0.2539, Val Acc: 91.39%\n",
      "Epoch [98/500], Train Loss: 0.0589, Train Acc: 98.12%, Val Loss: 1.2498, Val Acc: 64.41%\n",
      "Epoch [99/500], Train Loss: 0.0554, Train Acc: 98.17%, Val Loss: 0.8567, Val Acc: 73.74%\n",
      "Epoch [100/500], Train Loss: 0.0575, Train Acc: 98.16%, Val Loss: 0.1038, Val Acc: 96.97%\n",
      "Epoch [101/500], Train Loss: 0.0539, Train Acc: 98.21%, Val Loss: 0.1480, Val Acc: 95.53%\n",
      "Epoch [102/500], Train Loss: 0.0559, Train Acc: 98.20%, Val Loss: 0.3519, Val Acc: 89.43%\n",
      "Epoch [103/500], Train Loss: 0.0550, Train Acc: 98.27%, Val Loss: 0.1101, Val Acc: 96.79%\n",
      "Epoch [104/500], Train Loss: 0.0542, Train Acc: 98.28%, Val Loss: 0.0942, Val Acc: 97.25%\n",
      "Epoch [105/500], Train Loss: 0.0553, Train Acc: 98.26%, Val Loss: 1.1666, Val Acc: 68.36%\n",
      "Epoch [106/500], Train Loss: 0.0541, Train Acc: 98.22%, Val Loss: 0.7269, Val Acc: 79.78%\n",
      "Epoch [107/500], Train Loss: 0.0560, Train Acc: 98.20%, Val Loss: 0.1363, Val Acc: 96.03%\n",
      "Epoch [108/500], Train Loss: 0.0521, Train Acc: 98.33%, Val Loss: 0.4324, Val Acc: 85.96%\n",
      "Epoch [109/500], Train Loss: 0.0531, Train Acc: 98.29%, Val Loss: 0.0604, Val Acc: 98.17%\n",
      "Epoch [110/500], Train Loss: 0.0534, Train Acc: 98.22%, Val Loss: 0.0986, Val Acc: 97.18%\n",
      "Epoch [111/500], Train Loss: 0.0527, Train Acc: 98.30%, Val Loss: 0.0735, Val Acc: 97.80%\n",
      "Epoch [112/500], Train Loss: 0.0526, Train Acc: 98.31%, Val Loss: 0.0625, Val Acc: 98.19%\n",
      "Epoch [113/500], Train Loss: 0.0503, Train Acc: 98.42%, Val Loss: 0.0799, Val Acc: 97.66%\n",
      "Epoch [114/500], Train Loss: 0.0521, Train Acc: 98.37%, Val Loss: 0.0770, Val Acc: 97.78%\n",
      "Epoch [115/500], Train Loss: 0.0521, Train Acc: 98.30%, Val Loss: 0.0952, Val Acc: 97.20%\n",
      "Epoch [116/500], Train Loss: 0.0507, Train Acc: 98.36%, Val Loss: 0.4209, Val Acc: 86.20%\n",
      "Epoch [117/500], Train Loss: 0.0505, Train Acc: 98.35%, Val Loss: 0.6418, Val Acc: 79.28%\n",
      "Epoch [118/500], Train Loss: 0.0487, Train Acc: 98.49%, Val Loss: 0.3653, Val Acc: 88.33%\n",
      "Epoch [119/500], Train Loss: 0.0514, Train Acc: 98.38%, Val Loss: 0.0782, Val Acc: 97.77%\n",
      "Epoch [120/500], Train Loss: 0.0485, Train Acc: 98.42%, Val Loss: 0.0858, Val Acc: 97.30%\n",
      "Epoch [121/500], Train Loss: 0.0514, Train Acc: 98.28%, Val Loss: 0.1438, Val Acc: 95.87%\n",
      "Epoch [122/500], Train Loss: 0.0502, Train Acc: 98.44%, Val Loss: 0.3259, Val Acc: 89.67%\n",
      "Epoch [123/500], Train Loss: 0.0497, Train Acc: 98.38%, Val Loss: 0.4469, Val Acc: 86.05%\n",
      "Epoch [124/500], Train Loss: 0.0498, Train Acc: 98.46%, Val Loss: 0.0539, Val Acc: 98.40%\n",
      "Epoch [125/500], Train Loss: 0.0510, Train Acc: 98.42%, Val Loss: 0.0881, Val Acc: 97.33%\n",
      "Epoch [126/500], Train Loss: 0.0486, Train Acc: 98.44%, Val Loss: 0.0865, Val Acc: 97.47%\n",
      "Epoch [127/500], Train Loss: 0.0472, Train Acc: 98.42%, Val Loss: 0.0975, Val Acc: 97.15%\n",
      "Epoch [128/500], Train Loss: 0.0470, Train Acc: 98.49%, Val Loss: 0.8725, Val Acc: 75.85%\n",
      "Epoch [129/500], Train Loss: 0.0467, Train Acc: 98.53%, Val Loss: 0.0666, Val Acc: 97.94%\n",
      "Epoch [130/500], Train Loss: 0.0488, Train Acc: 98.41%, Val Loss: 0.1668, Val Acc: 94.89%\n",
      "Epoch [131/500], Train Loss: 0.0473, Train Acc: 98.47%, Val Loss: 1.5816, Val Acc: 56.78%\n",
      "Epoch [132/500], Train Loss: 0.0471, Train Acc: 98.46%, Val Loss: 0.0981, Val Acc: 97.20%\n",
      "Epoch [133/500], Train Loss: 0.0446, Train Acc: 98.53%, Val Loss: 0.0609, Val Acc: 98.28%\n",
      "Epoch [134/500], Train Loss: 0.0451, Train Acc: 98.50%, Val Loss: 2.7942, Val Acc: 41.42%\n",
      "Epoch [135/500], Train Loss: 0.0458, Train Acc: 98.51%, Val Loss: 0.1055, Val Acc: 96.88%\n",
      "Epoch [136/500], Train Loss: 0.0465, Train Acc: 98.54%, Val Loss: 0.2061, Val Acc: 93.71%\n",
      "Epoch [137/500], Train Loss: 0.0432, Train Acc: 98.59%, Val Loss: 0.1187, Val Acc: 96.75%\n",
      "Epoch [138/500], Train Loss: 0.0470, Train Acc: 98.45%, Val Loss: 0.0564, Val Acc: 98.34%\n",
      "Epoch [139/500], Train Loss: 0.0448, Train Acc: 98.47%, Val Loss: 2.5918, Val Acc: 39.28%\n",
      "Epoch [140/500], Train Loss: 0.0438, Train Acc: 98.60%, Val Loss: 0.6870, Val Acc: 78.59%\n",
      "Epoch [141/500], Train Loss: 0.0447, Train Acc: 98.52%, Val Loss: 0.1683, Val Acc: 95.40%\n",
      "Epoch [142/500], Train Loss: 0.0438, Train Acc: 98.53%, Val Loss: 0.0761, Val Acc: 97.67%\n",
      "Epoch [143/500], Train Loss: 0.0449, Train Acc: 98.58%, Val Loss: 0.1211, Val Acc: 96.35%\n",
      "Epoch [144/500], Train Loss: 0.0463, Train Acc: 98.48%, Val Loss: 0.9657, Val Acc: 71.05%\n",
      "Epoch [145/500], Train Loss: 0.0426, Train Acc: 98.62%, Val Loss: 1.5284, Val Acc: 59.98%\n",
      "Epoch [146/500], Train Loss: 0.0451, Train Acc: 98.53%, Val Loss: 0.3397, Val Acc: 89.90%\n",
      "Epoch [147/500], Train Loss: 0.0448, Train Acc: 98.53%, Val Loss: 0.1272, Val Acc: 96.12%\n",
      "Epoch [148/500], Train Loss: 0.0436, Train Acc: 98.57%, Val Loss: 0.9033, Val Acc: 72.58%\n",
      "Epoch [149/500], Train Loss: 0.0423, Train Acc: 98.62%, Val Loss: 0.0585, Val Acc: 98.36%\n",
      "Epoch [150/500], Train Loss: 0.0437, Train Acc: 98.64%, Val Loss: 0.4841, Val Acc: 85.12%\n",
      "Epoch [151/500], Train Loss: 0.0439, Train Acc: 98.56%, Val Loss: 0.1228, Val Acc: 96.22%\n",
      "Epoch [152/500], Train Loss: 0.0436, Train Acc: 98.60%, Val Loss: 0.3034, Val Acc: 89.89%\n",
      "Epoch [153/500], Train Loss: 0.0434, Train Acc: 98.60%, Val Loss: 0.0770, Val Acc: 97.72%\n",
      "Epoch [154/500], Train Loss: 0.0422, Train Acc: 98.65%, Val Loss: 0.3524, Val Acc: 88.76%\n",
      "Epoch [155/500], Train Loss: 0.0422, Train Acc: 98.62%, Val Loss: 0.0719, Val Acc: 97.98%\n",
      "Epoch [156/500], Train Loss: 0.0425, Train Acc: 98.66%, Val Loss: 0.0693, Val Acc: 97.97%\n",
      "Epoch [157/500], Train Loss: 0.0440, Train Acc: 98.50%, Val Loss: 0.3488, Val Acc: 88.93%\n",
      "Epoch [158/500], Train Loss: 0.0394, Train Acc: 98.68%, Val Loss: 0.1893, Val Acc: 94.44%\n",
      "Epoch [159/500], Train Loss: 0.0399, Train Acc: 98.68%, Val Loss: 0.1042, Val Acc: 96.78%\n",
      "Epoch [160/500], Train Loss: 0.0410, Train Acc: 98.65%, Val Loss: 0.0550, Val Acc: 98.42%\n",
      "Epoch [161/500], Train Loss: 0.0406, Train Acc: 98.65%, Val Loss: 0.0662, Val Acc: 98.07%\n",
      "Epoch [162/500], Train Loss: 0.0401, Train Acc: 98.66%, Val Loss: 0.3977, Val Acc: 87.12%\n",
      "Epoch [163/500], Train Loss: 0.0416, Train Acc: 98.60%, Val Loss: 0.0508, Val Acc: 98.38%\n",
      "Epoch [164/500], Train Loss: 0.0428, Train Acc: 98.64%, Val Loss: 0.7487, Val Acc: 78.06%\n",
      "Epoch [165/500], Train Loss: 0.0405, Train Acc: 98.66%, Val Loss: 0.0810, Val Acc: 97.51%\n",
      "Epoch [166/500], Train Loss: 0.0394, Train Acc: 98.69%, Val Loss: 1.2840, Val Acc: 68.39%\n",
      "Epoch [167/500], Train Loss: 0.0410, Train Acc: 98.64%, Val Loss: 0.0788, Val Acc: 97.69%\n",
      "Epoch [168/500], Train Loss: 0.0415, Train Acc: 98.63%, Val Loss: 0.0590, Val Acc: 98.36%\n",
      "Epoch [169/500], Train Loss: 0.0387, Train Acc: 98.73%, Val Loss: 0.4379, Val Acc: 86.28%\n",
      "Epoch [170/500], Train Loss: 0.0406, Train Acc: 98.65%, Val Loss: 0.0856, Val Acc: 97.43%\n",
      "Epoch [171/500], Train Loss: 0.0390, Train Acc: 98.70%, Val Loss: 0.1835, Val Acc: 94.22%\n",
      "Epoch [172/500], Train Loss: 0.0408, Train Acc: 98.68%, Val Loss: 0.0810, Val Acc: 97.68%\n",
      "Epoch [173/500], Train Loss: 0.0386, Train Acc: 98.70%, Val Loss: 0.0997, Val Acc: 96.87%\n",
      "Epoch [174/500], Train Loss: 0.0378, Train Acc: 98.69%, Val Loss: 1.9068, Val Acc: 51.38%\n",
      "Epoch [175/500], Train Loss: 0.0399, Train Acc: 98.68%, Val Loss: 0.0511, Val Acc: 98.44%\n",
      "Epoch [176/500], Train Loss: 0.0395, Train Acc: 98.69%, Val Loss: 0.0730, Val Acc: 97.87%\n",
      "Epoch [177/500], Train Loss: 0.0380, Train Acc: 98.76%, Val Loss: 0.1523, Val Acc: 95.33%\n",
      "Epoch [178/500], Train Loss: 0.0367, Train Acc: 98.80%, Val Loss: 0.4070, Val Acc: 87.02%\n",
      "Epoch [179/500], Train Loss: 0.0381, Train Acc: 98.73%, Val Loss: 0.0529, Val Acc: 98.58%\n",
      "Epoch [180/500], Train Loss: 0.0396, Train Acc: 98.74%, Val Loss: 0.4883, Val Acc: 86.35%\n",
      "Epoch [181/500], Train Loss: 0.0376, Train Acc: 98.74%, Val Loss: 0.2323, Val Acc: 92.50%\n",
      "Epoch [182/500], Train Loss: 0.0376, Train Acc: 98.83%, Val Loss: 0.1573, Val Acc: 95.20%\n",
      "Epoch [183/500], Train Loss: 0.0375, Train Acc: 98.76%, Val Loss: 0.1290, Val Acc: 95.74%\n",
      "Epoch [184/500], Train Loss: 0.0366, Train Acc: 98.77%, Val Loss: 0.2293, Val Acc: 93.05%\n",
      "Epoch [185/500], Train Loss: 0.0394, Train Acc: 98.74%, Val Loss: 0.7075, Val Acc: 78.03%\n",
      "Epoch [186/500], Train Loss: 0.0393, Train Acc: 98.73%, Val Loss: 0.8259, Val Acc: 76.76%\n",
      "Epoch [187/500], Train Loss: 0.0380, Train Acc: 98.72%, Val Loss: 0.0448, Val Acc: 98.68%\n",
      "Epoch [188/500], Train Loss: 0.0374, Train Acc: 98.79%, Val Loss: 0.1202, Val Acc: 96.38%\n",
      "Epoch [189/500], Train Loss: 0.0386, Train Acc: 98.77%, Val Loss: 0.6102, Val Acc: 83.03%\n",
      "Epoch [190/500], Train Loss: 0.0387, Train Acc: 98.72%, Val Loss: 0.7096, Val Acc: 79.55%\n",
      "Epoch [191/500], Train Loss: 0.0361, Train Acc: 98.78%, Val Loss: 0.1228, Val Acc: 96.29%\n",
      "Epoch [192/500], Train Loss: 0.0358, Train Acc: 98.83%, Val Loss: 0.6970, Val Acc: 81.28%\n",
      "Epoch [193/500], Train Loss: 0.0366, Train Acc: 98.79%, Val Loss: 0.2401, Val Acc: 92.17%\n",
      "Epoch [194/500], Train Loss: 0.0376, Train Acc: 98.79%, Val Loss: 0.0763, Val Acc: 97.75%\n",
      "Epoch [195/500], Train Loss: 0.0368, Train Acc: 98.83%, Val Loss: 0.0686, Val Acc: 97.98%\n",
      "Epoch [196/500], Train Loss: 0.0365, Train Acc: 98.82%, Val Loss: 0.0935, Val Acc: 97.38%\n",
      "Epoch [197/500], Train Loss: 0.0336, Train Acc: 98.87%, Val Loss: 0.4817, Val Acc: 85.99%\n",
      "Epoch [198/500], Train Loss: 0.0364, Train Acc: 98.82%, Val Loss: 0.6006, Val Acc: 84.53%\n",
      "Epoch [199/500], Train Loss: 0.0356, Train Acc: 98.84%, Val Loss: 0.1068, Val Acc: 96.68%\n",
      "Epoch [200/500], Train Loss: 0.0359, Train Acc: 98.82%, Val Loss: 0.1572, Val Acc: 95.24%\n",
      "Epoch [201/500], Train Loss: 0.0358, Train Acc: 98.82%, Val Loss: 0.0917, Val Acc: 97.30%\n",
      "Epoch [202/500], Train Loss: 0.0339, Train Acc: 98.85%, Val Loss: 0.0504, Val Acc: 98.60%\n",
      "Epoch [203/500], Train Loss: 0.0356, Train Acc: 98.84%, Val Loss: 0.0754, Val Acc: 97.79%\n",
      "Epoch [204/500], Train Loss: 0.0361, Train Acc: 98.83%, Val Loss: 0.0921, Val Acc: 97.25%\n",
      "Epoch [205/500], Train Loss: 0.0338, Train Acc: 98.85%, Val Loss: 0.0453, Val Acc: 98.66%\n",
      "Epoch [206/500], Train Loss: 0.0353, Train Acc: 98.85%, Val Loss: 0.4049, Val Acc: 87.81%\n",
      "Epoch [207/500], Train Loss: 0.0344, Train Acc: 98.84%, Val Loss: 0.2202, Val Acc: 93.03%\n",
      "Epoch [208/500], Train Loss: 0.0337, Train Acc: 98.89%, Val Loss: 0.0563, Val Acc: 98.46%\n",
      "Epoch [209/500], Train Loss: 0.0353, Train Acc: 98.83%, Val Loss: 0.0572, Val Acc: 98.31%\n",
      "Epoch [210/500], Train Loss: 0.0340, Train Acc: 98.87%, Val Loss: 0.0654, Val Acc: 98.17%\n",
      "Epoch [211/500], Train Loss: 0.0358, Train Acc: 98.84%, Val Loss: 0.1546, Val Acc: 95.24%\n",
      "Epoch [212/500], Train Loss: 0.0327, Train Acc: 98.89%, Val Loss: 0.0574, Val Acc: 98.22%\n",
      "Epoch [213/500], Train Loss: 0.0338, Train Acc: 98.85%, Val Loss: 0.5679, Val Acc: 83.97%\n",
      "Epoch [214/500], Train Loss: 0.0336, Train Acc: 98.89%, Val Loss: 0.0530, Val Acc: 98.47%\n",
      "Epoch [215/500], Train Loss: 0.0342, Train Acc: 98.86%, Val Loss: 0.1264, Val Acc: 96.13%\n",
      "Epoch [216/500], Train Loss: 0.0341, Train Acc: 98.89%, Val Loss: 0.1737, Val Acc: 94.58%\n",
      "Epoch [217/500], Train Loss: 0.0328, Train Acc: 98.92%, Val Loss: 0.1821, Val Acc: 94.99%\n",
      "Epoch [218/500], Train Loss: 0.0345, Train Acc: 98.86%, Val Loss: 0.0547, Val Acc: 98.33%\n",
      "Epoch [219/500], Train Loss: 0.0325, Train Acc: 98.90%, Val Loss: 0.0546, Val Acc: 98.60%\n",
      "Epoch [220/500], Train Loss: 0.0353, Train Acc: 98.74%, Val Loss: 0.0603, Val Acc: 98.28%\n",
      "Epoch [221/500], Train Loss: 0.0325, Train Acc: 98.88%, Val Loss: 0.1336, Val Acc: 96.08%\n",
      "Epoch [222/500], Train Loss: 0.0348, Train Acc: 98.89%, Val Loss: 0.0471, Val Acc: 98.57%\n",
      "Epoch [223/500], Train Loss: 0.0337, Train Acc: 98.88%, Val Loss: 0.0796, Val Acc: 97.70%\n",
      "Epoch [224/500], Train Loss: 0.0337, Train Acc: 98.91%, Val Loss: 0.0537, Val Acc: 98.48%\n",
      "Epoch [225/500], Train Loss: 0.0331, Train Acc: 98.90%, Val Loss: 0.7925, Val Acc: 78.27%\n",
      "Epoch [226/500], Train Loss: 0.0326, Train Acc: 98.90%, Val Loss: 0.0933, Val Acc: 97.19%\n",
      "Epoch [227/500], Train Loss: 0.0330, Train Acc: 98.91%, Val Loss: 0.2023, Val Acc: 94.26%\n",
      "Epoch [228/500], Train Loss: 0.0314, Train Acc: 98.95%, Val Loss: 0.0607, Val Acc: 98.43%\n",
      "Epoch [229/500], Train Loss: 0.0314, Train Acc: 98.95%, Val Loss: 0.0499, Val Acc: 98.56%\n",
      "Epoch [230/500], Train Loss: 0.0312, Train Acc: 98.97%, Val Loss: 0.0502, Val Acc: 98.51%\n",
      "Epoch [231/500], Train Loss: 0.0334, Train Acc: 98.92%, Val Loss: 0.0468, Val Acc: 98.66%\n",
      "Epoch [232/500], Train Loss: 0.0304, Train Acc: 99.02%, Val Loss: 0.0555, Val Acc: 98.46%\n",
      "Epoch [233/500], Train Loss: 0.0316, Train Acc: 99.00%, Val Loss: 0.5316, Val Acc: 84.85%\n",
      "Epoch [234/500], Train Loss: 0.0337, Train Acc: 98.87%, Val Loss: 0.1087, Val Acc: 96.74%\n",
      "Epoch [235/500], Train Loss: 0.0310, Train Acc: 99.03%, Val Loss: 0.0643, Val Acc: 98.25%\n",
      "Epoch [236/500], Train Loss: 0.0301, Train Acc: 98.97%, Val Loss: 0.0557, Val Acc: 98.40%\n",
      "Epoch [237/500], Train Loss: 0.0321, Train Acc: 98.95%, Val Loss: 0.1027, Val Acc: 96.90%\n",
      "Epoch [238/500], Train Loss: 0.0326, Train Acc: 98.93%, Val Loss: 0.1548, Val Acc: 95.46%\n",
      "Epoch [239/500], Train Loss: 0.0304, Train Acc: 98.96%, Val Loss: 0.0494, Val Acc: 98.57%\n",
      "Epoch [240/500], Train Loss: 0.0325, Train Acc: 98.92%, Val Loss: 0.0462, Val Acc: 98.65%\n",
      "Epoch [241/500], Train Loss: 0.0300, Train Acc: 98.98%, Val Loss: 0.0838, Val Acc: 97.68%\n",
      "Epoch [242/500], Train Loss: 0.0297, Train Acc: 99.02%, Val Loss: 0.2513, Val Acc: 92.41%\n",
      "Epoch [243/500], Train Loss: 0.0302, Train Acc: 99.01%, Val Loss: 0.0610, Val Acc: 98.31%\n",
      "Epoch [244/500], Train Loss: 0.0291, Train Acc: 99.02%, Val Loss: 0.0472, Val Acc: 98.68%\n",
      "Epoch [245/500], Train Loss: 0.0297, Train Acc: 99.03%, Val Loss: 0.2762, Val Acc: 91.19%\n",
      "Epoch [246/500], Train Loss: 0.0336, Train Acc: 98.89%, Val Loss: 0.0827, Val Acc: 97.67%\n",
      "Epoch [247/500], Train Loss: 0.0303, Train Acc: 98.97%, Val Loss: 0.0763, Val Acc: 97.97%\n",
      "Epoch [248/500], Train Loss: 0.0297, Train Acc: 98.92%, Val Loss: 0.0468, Val Acc: 98.76%\n",
      "Epoch [249/500], Train Loss: 0.0309, Train Acc: 99.00%, Val Loss: 0.4472, Val Acc: 86.23%\n",
      "Epoch [250/500], Train Loss: 0.0288, Train Acc: 99.03%, Val Loss: 1.1017, Val Acc: 69.22%\n",
      "Epoch [251/500], Train Loss: 0.0312, Train Acc: 98.97%, Val Loss: 0.0529, Val Acc: 98.42%\n",
      "Epoch [252/500], Train Loss: 0.0316, Train Acc: 98.96%, Val Loss: 0.0460, Val Acc: 98.73%\n",
      "Epoch [253/500], Train Loss: 0.0296, Train Acc: 99.02%, Val Loss: 0.0668, Val Acc: 97.91%\n",
      "Epoch [254/500], Train Loss: 0.0310, Train Acc: 98.97%, Val Loss: 0.4250, Val Acc: 87.96%\n",
      "Epoch [255/500], Train Loss: 0.0303, Train Acc: 99.00%, Val Loss: 0.0446, Val Acc: 98.80%\n",
      "Epoch [256/500], Train Loss: 0.0300, Train Acc: 99.04%, Val Loss: 0.0470, Val Acc: 98.58%\n",
      "Epoch [257/500], Train Loss: 0.0312, Train Acc: 98.91%, Val Loss: 0.0609, Val Acc: 98.26%\n",
      "Epoch [258/500], Train Loss: 0.0298, Train Acc: 98.97%, Val Loss: 0.1363, Val Acc: 96.03%\n",
      "Epoch [259/500], Train Loss: 0.0294, Train Acc: 98.96%, Val Loss: 0.0563, Val Acc: 98.55%\n",
      "Epoch [260/500], Train Loss: 0.0290, Train Acc: 99.02%, Val Loss: 0.1857, Val Acc: 94.55%\n",
      "Epoch [261/500], Train Loss: 0.0308, Train Acc: 99.01%, Val Loss: 0.0557, Val Acc: 98.46%\n",
      "Epoch [262/500], Train Loss: 0.0289, Train Acc: 99.03%, Val Loss: 0.0569, Val Acc: 98.42%\n",
      "Epoch [263/500], Train Loss: 0.0321, Train Acc: 98.93%, Val Loss: 0.0522, Val Acc: 98.52%\n",
      "Epoch [264/500], Train Loss: 0.0298, Train Acc: 98.99%, Val Loss: 0.0792, Val Acc: 97.70%\n",
      "Epoch [265/500], Train Loss: 0.0292, Train Acc: 98.99%, Val Loss: 0.0575, Val Acc: 98.43%\n",
      "Epoch [266/500], Train Loss: 0.0300, Train Acc: 98.97%, Val Loss: 0.0515, Val Acc: 98.53%\n",
      "Epoch [267/500], Train Loss: 0.0294, Train Acc: 99.01%, Val Loss: 0.0628, Val Acc: 98.14%\n",
      "Epoch [268/500], Train Loss: 0.0291, Train Acc: 99.04%, Val Loss: 0.3088, Val Acc: 91.95%\n",
      "Epoch [269/500], Train Loss: 0.0303, Train Acc: 98.99%, Val Loss: 0.0525, Val Acc: 98.61%\n",
      "Epoch [270/500], Train Loss: 0.0280, Train Acc: 99.07%, Val Loss: 0.0936, Val Acc: 97.22%\n",
      "Epoch [271/500], Train Loss: 0.0279, Train Acc: 99.04%, Val Loss: 0.0541, Val Acc: 98.33%\n",
      "Epoch [272/500], Train Loss: 0.0301, Train Acc: 98.98%, Val Loss: 1.2459, Val Acc: 63.86%\n",
      "Epoch [273/500], Train Loss: 0.0278, Train Acc: 99.12%, Val Loss: 0.0478, Val Acc: 98.61%\n",
      "Epoch [274/500], Train Loss: 0.0306, Train Acc: 98.96%, Val Loss: 0.0534, Val Acc: 98.47%\n",
      "Epoch [275/500], Train Loss: 0.0294, Train Acc: 99.03%, Val Loss: 0.0695, Val Acc: 97.93%\n",
      "Epoch [276/500], Train Loss: 0.0272, Train Acc: 99.06%, Val Loss: 0.0524, Val Acc: 98.43%\n",
      "Epoch [277/500], Train Loss: 0.0289, Train Acc: 99.00%, Val Loss: 0.0558, Val Acc: 98.45%\n",
      "Epoch [278/500], Train Loss: 0.0287, Train Acc: 98.99%, Val Loss: 0.0472, Val Acc: 98.67%\n",
      "Epoch [279/500], Train Loss: 0.0272, Train Acc: 99.11%, Val Loss: 0.0554, Val Acc: 98.39%\n",
      "Epoch [280/500], Train Loss: 0.0284, Train Acc: 99.06%, Val Loss: 0.0617, Val Acc: 98.22%\n",
      "Epoch [281/500], Train Loss: 0.0287, Train Acc: 99.06%, Val Loss: 0.0773, Val Acc: 97.83%\n",
      "Epoch [282/500], Train Loss: 0.0286, Train Acc: 99.08%, Val Loss: 0.1091, Val Acc: 96.68%\n",
      "Epoch [283/500], Train Loss: 0.0311, Train Acc: 98.96%, Val Loss: 0.2517, Val Acc: 92.28%\n",
      "Epoch [284/500], Train Loss: 0.0283, Train Acc: 99.02%, Val Loss: 0.0923, Val Acc: 97.23%\n",
      "Epoch [285/500], Train Loss: 0.0313, Train Acc: 98.97%, Val Loss: 0.0698, Val Acc: 98.01%\n",
      "Epoch [286/500], Train Loss: 0.0299, Train Acc: 99.00%, Val Loss: 0.0810, Val Acc: 97.70%\n",
      "Epoch [287/500], Train Loss: 0.0284, Train Acc: 99.04%, Val Loss: 0.0475, Val Acc: 98.64%\n",
      "Epoch [288/500], Train Loss: 0.0288, Train Acc: 99.01%, Val Loss: 0.0757, Val Acc: 97.82%\n",
      "Epoch [289/500], Train Loss: 0.0277, Train Acc: 99.06%, Val Loss: 0.0500, Val Acc: 98.58%\n",
      "Epoch [290/500], Train Loss: 0.0278, Train Acc: 99.07%, Val Loss: 0.0697, Val Acc: 97.97%\n",
      "Epoch [291/500], Train Loss: 0.0276, Train Acc: 99.07%, Val Loss: 0.0454, Val Acc: 98.78%\n",
      "Epoch [292/500], Train Loss: 0.0277, Train Acc: 99.07%, Val Loss: 0.0802, Val Acc: 97.82%\n",
      "Epoch [293/500], Train Loss: 0.0271, Train Acc: 99.10%, Val Loss: 0.0506, Val Acc: 98.54%\n",
      "Epoch [294/500], Train Loss: 0.0278, Train Acc: 99.07%, Val Loss: 0.0980, Val Acc: 97.08%\n",
      "Epoch [295/500], Train Loss: 0.0264, Train Acc: 99.12%, Val Loss: 0.0492, Val Acc: 98.67%\n",
      "Epoch [296/500], Train Loss: 0.0274, Train Acc: 99.10%, Val Loss: 0.0459, Val Acc: 98.73%\n",
      "Epoch [297/500], Train Loss: 0.0267, Train Acc: 99.06%, Val Loss: 0.1036, Val Acc: 97.06%\n",
      "Epoch [298/500], Train Loss: 0.0261, Train Acc: 99.09%, Val Loss: 0.0553, Val Acc: 98.49%\n",
      "Epoch [299/500], Train Loss: 0.0272, Train Acc: 99.09%, Val Loss: 0.0437, Val Acc: 98.67%\n",
      "Epoch [300/500], Train Loss: 0.0276, Train Acc: 99.05%, Val Loss: 0.0533, Val Acc: 98.55%\n",
      "Epoch [301/500], Train Loss: 0.0263, Train Acc: 99.17%, Val Loss: 0.4878, Val Acc: 86.03%\n",
      "Epoch [302/500], Train Loss: 0.0278, Train Acc: 99.06%, Val Loss: 0.0829, Val Acc: 97.53%\n",
      "Epoch [303/500], Train Loss: 0.0269, Train Acc: 99.08%, Val Loss: 0.0618, Val Acc: 98.22%\n",
      "Epoch [304/500], Train Loss: 0.0270, Train Acc: 99.08%, Val Loss: 0.0521, Val Acc: 98.53%\n",
      "Epoch [305/500], Train Loss: 0.0279, Train Acc: 99.11%, Val Loss: 0.0836, Val Acc: 97.59%\n",
      "Epoch [306/500], Train Loss: 0.0265, Train Acc: 99.08%, Val Loss: 0.1377, Val Acc: 95.86%\n",
      "Epoch [307/500], Train Loss: 0.0276, Train Acc: 99.10%, Val Loss: 0.0503, Val Acc: 98.58%\n",
      "Epoch [308/500], Train Loss: 0.0263, Train Acc: 99.11%, Val Loss: 0.0477, Val Acc: 98.62%\n",
      "Epoch [309/500], Train Loss: 0.0265, Train Acc: 99.15%, Val Loss: 0.0466, Val Acc: 98.76%\n",
      "Epoch [310/500], Train Loss: 0.0257, Train Acc: 99.12%, Val Loss: 0.0456, Val Acc: 98.67%\n",
      "Epoch [311/500], Train Loss: 0.0263, Train Acc: 99.14%, Val Loss: 0.0498, Val Acc: 98.74%\n",
      "Epoch [312/500], Train Loss: 0.0263, Train Acc: 99.09%, Val Loss: 0.0752, Val Acc: 97.98%\n",
      "Epoch [313/500], Train Loss: 0.0274, Train Acc: 99.07%, Val Loss: 0.3772, Val Acc: 88.43%\n",
      "Epoch [314/500], Train Loss: 0.0257, Train Acc: 99.15%, Val Loss: 0.0901, Val Acc: 97.45%\n",
      "Epoch [315/500], Train Loss: 0.0242, Train Acc: 99.16%, Val Loss: 0.0509, Val Acc: 98.61%\n",
      "Epoch [316/500], Train Loss: 0.0262, Train Acc: 99.12%, Val Loss: 0.2682, Val Acc: 91.78%\n",
      "Epoch [317/500], Train Loss: 0.0269, Train Acc: 99.10%, Val Loss: 0.0464, Val Acc: 98.78%\n",
      "Epoch [318/500], Train Loss: 0.0261, Train Acc: 99.12%, Val Loss: 0.0786, Val Acc: 97.84%\n",
      "Epoch [319/500], Train Loss: 0.0259, Train Acc: 99.13%, Val Loss: 0.1061, Val Acc: 96.78%\n",
      "Epoch [320/500], Train Loss: 0.0275, Train Acc: 99.04%, Val Loss: 0.0523, Val Acc: 98.64%\n",
      "Epoch [321/500], Train Loss: 0.0254, Train Acc: 99.15%, Val Loss: 0.0456, Val Acc: 98.71%\n",
      "Epoch [322/500], Train Loss: 0.0247, Train Acc: 99.22%, Val Loss: 0.1049, Val Acc: 96.92%\n",
      "Epoch [323/500], Train Loss: 0.0246, Train Acc: 99.21%, Val Loss: 0.0466, Val Acc: 98.77%\n",
      "Epoch [324/500], Train Loss: 0.0264, Train Acc: 99.14%, Val Loss: 0.0460, Val Acc: 98.68%\n",
      "Epoch [325/500], Train Loss: 0.0257, Train Acc: 99.17%, Val Loss: 0.0492, Val Acc: 98.65%\n",
      "Epoch [326/500], Train Loss: 0.0243, Train Acc: 99.16%, Val Loss: 0.0508, Val Acc: 98.58%\n",
      "Epoch [327/500], Train Loss: 0.0240, Train Acc: 99.17%, Val Loss: 0.1164, Val Acc: 96.43%\n",
      "Epoch [328/500], Train Loss: 0.0264, Train Acc: 99.11%, Val Loss: 0.0489, Val Acc: 98.63%\n",
      "Epoch [329/500], Train Loss: 0.0245, Train Acc: 99.18%, Val Loss: 0.0802, Val Acc: 97.89%\n",
      "Epoch [330/500], Train Loss: 0.0248, Train Acc: 99.15%, Val Loss: 0.0469, Val Acc: 98.62%\n",
      "Epoch [331/500], Train Loss: 0.0241, Train Acc: 99.19%, Val Loss: 0.1034, Val Acc: 97.21%\n",
      "Epoch [332/500], Train Loss: 0.0251, Train Acc: 99.12%, Val Loss: 0.0453, Val Acc: 98.72%\n",
      "Epoch [333/500], Train Loss: 0.0245, Train Acc: 99.19%, Val Loss: 0.0626, Val Acc: 98.30%\n",
      "Epoch [334/500], Train Loss: 0.0242, Train Acc: 99.20%, Val Loss: 0.6653, Val Acc: 82.63%\n",
      "Epoch [335/500], Train Loss: 0.0263, Train Acc: 99.11%, Val Loss: 0.1225, Val Acc: 96.74%\n",
      "Epoch [336/500], Train Loss: 0.0237, Train Acc: 99.19%, Val Loss: 0.1666, Val Acc: 95.03%\n",
      "Epoch [337/500], Train Loss: 0.0239, Train Acc: 99.21%, Val Loss: 0.7650, Val Acc: 80.53%\n",
      "Epoch [338/500], Train Loss: 0.0249, Train Acc: 99.15%, Val Loss: 0.0510, Val Acc: 98.63%\n",
      "Epoch [339/500], Train Loss: 0.0234, Train Acc: 99.20%, Val Loss: 0.0635, Val Acc: 98.24%\n",
      "Epoch [340/500], Train Loss: 0.0253, Train Acc: 99.15%, Val Loss: 0.0509, Val Acc: 98.58%\n",
      "Epoch [341/500], Train Loss: 0.0234, Train Acc: 99.21%, Val Loss: 0.0576, Val Acc: 98.54%\n",
      "Epoch [342/500], Train Loss: 0.0255, Train Acc: 99.15%, Val Loss: 0.0538, Val Acc: 98.40%\n",
      "Epoch [343/500], Train Loss: 0.0248, Train Acc: 99.17%, Val Loss: 0.0628, Val Acc: 98.22%\n",
      "Epoch [344/500], Train Loss: 0.0247, Train Acc: 99.19%, Val Loss: 0.0493, Val Acc: 98.56%\n",
      "Epoch [345/500], Train Loss: 0.0238, Train Acc: 99.17%, Val Loss: 0.0679, Val Acc: 98.12%\n",
      "Epoch [346/500], Train Loss: 0.0252, Train Acc: 99.15%, Val Loss: 0.0520, Val Acc: 98.52%\n",
      "Epoch [347/500], Train Loss: 0.0249, Train Acc: 99.14%, Val Loss: 0.0567, Val Acc: 98.52%\n",
      "Epoch [348/500], Train Loss: 0.0241, Train Acc: 99.17%, Val Loss: 0.1144, Val Acc: 96.59%\n",
      "Epoch [349/500], Train Loss: 0.0236, Train Acc: 99.20%, Val Loss: 0.0489, Val Acc: 98.58%\n",
      "Epoch [350/500], Train Loss: 0.0232, Train Acc: 99.20%, Val Loss: 0.0441, Val Acc: 98.82%\n",
      "Epoch [351/500], Train Loss: 0.0246, Train Acc: 99.18%, Val Loss: 0.2424, Val Acc: 92.38%\n",
      "Epoch [352/500], Train Loss: 0.0232, Train Acc: 99.23%, Val Loss: 0.0417, Val Acc: 98.93%\n",
      "Epoch [353/500], Train Loss: 0.0240, Train Acc: 99.17%, Val Loss: 0.0530, Val Acc: 98.48%\n",
      "Epoch [354/500], Train Loss: 0.0241, Train Acc: 99.20%, Val Loss: 0.1054, Val Acc: 97.09%\n",
      "Epoch [355/500], Train Loss: 0.0237, Train Acc: 99.19%, Val Loss: 0.1082, Val Acc: 96.89%\n",
      "Epoch [356/500], Train Loss: 0.0219, Train Acc: 99.26%, Val Loss: 0.0459, Val Acc: 98.82%\n",
      "Epoch [357/500], Train Loss: 0.0247, Train Acc: 99.13%, Val Loss: 0.0477, Val Acc: 98.76%\n",
      "Epoch [358/500], Train Loss: 0.0247, Train Acc: 99.16%, Val Loss: 0.1015, Val Acc: 96.94%\n",
      "Epoch [359/500], Train Loss: 0.0232, Train Acc: 99.21%, Val Loss: 0.0922, Val Acc: 97.29%\n",
      "Epoch [360/500], Train Loss: 0.0233, Train Acc: 99.21%, Val Loss: 0.1737, Val Acc: 94.99%\n",
      "Epoch [361/500], Train Loss: 0.0238, Train Acc: 99.22%, Val Loss: 0.0444, Val Acc: 98.77%\n",
      "Epoch [362/500], Train Loss: 0.0224, Train Acc: 99.26%, Val Loss: 0.1483, Val Acc: 95.72%\n",
      "Epoch [363/500], Train Loss: 0.0245, Train Acc: 99.18%, Val Loss: 0.0534, Val Acc: 98.39%\n",
      "Epoch [364/500], Train Loss: 0.0234, Train Acc: 99.24%, Val Loss: 0.0854, Val Acc: 97.62%\n",
      "Epoch [365/500], Train Loss: 0.0250, Train Acc: 99.15%, Val Loss: 0.0542, Val Acc: 98.49%\n",
      "Epoch [366/500], Train Loss: 0.0224, Train Acc: 99.25%, Val Loss: 0.0462, Val Acc: 98.69%\n",
      "Epoch [367/500], Train Loss: 0.0229, Train Acc: 99.21%, Val Loss: 0.0498, Val Acc: 98.59%\n",
      "Epoch [368/500], Train Loss: 0.0235, Train Acc: 99.20%, Val Loss: 0.0834, Val Acc: 97.70%\n",
      "Epoch [369/500], Train Loss: 0.0233, Train Acc: 99.24%, Val Loss: 0.0531, Val Acc: 98.63%\n",
      "Epoch [370/500], Train Loss: 0.0233, Train Acc: 99.23%, Val Loss: 0.0491, Val Acc: 98.58%\n",
      "Epoch [371/500], Train Loss: 0.0234, Train Acc: 99.21%, Val Loss: 0.0458, Val Acc: 98.72%\n",
      "Epoch [372/500], Train Loss: 0.0229, Train Acc: 99.22%, Val Loss: 0.0445, Val Acc: 98.88%\n",
      "Epoch [373/500], Train Loss: 0.0227, Train Acc: 99.24%, Val Loss: 0.0738, Val Acc: 97.92%\n",
      "Epoch [374/500], Train Loss: 0.0226, Train Acc: 99.25%, Val Loss: 0.0505, Val Acc: 98.61%\n",
      "Epoch [375/500], Train Loss: 0.0213, Train Acc: 99.25%, Val Loss: 0.0537, Val Acc: 98.47%\n",
      "Epoch [376/500], Train Loss: 0.0230, Train Acc: 99.25%, Val Loss: 0.0435, Val Acc: 98.78%\n",
      "Epoch [377/500], Train Loss: 0.0240, Train Acc: 99.17%, Val Loss: 0.3257, Val Acc: 90.67%\n",
      "Epoch [378/500], Train Loss: 0.0229, Train Acc: 99.21%, Val Loss: 0.0447, Val Acc: 98.82%\n",
      "Epoch [379/500], Train Loss: 0.0225, Train Acc: 99.20%, Val Loss: 0.0424, Val Acc: 98.84%\n",
      "Epoch [380/500], Train Loss: 0.0226, Train Acc: 99.25%, Val Loss: 0.0519, Val Acc: 98.63%\n",
      "Epoch [381/500], Train Loss: 0.0224, Train Acc: 99.24%, Val Loss: 0.0449, Val Acc: 98.83%\n",
      "Epoch [382/500], Train Loss: 0.0231, Train Acc: 99.23%, Val Loss: 0.0584, Val Acc: 98.36%\n",
      "Epoch [383/500], Train Loss: 0.0228, Train Acc: 99.21%, Val Loss: 0.0881, Val Acc: 97.60%\n",
      "Epoch [384/500], Train Loss: 0.0235, Train Acc: 99.21%, Val Loss: 0.0441, Val Acc: 98.82%\n",
      "Epoch [385/500], Train Loss: 0.0229, Train Acc: 99.17%, Val Loss: 0.0480, Val Acc: 98.65%\n",
      "Epoch [386/500], Train Loss: 0.0226, Train Acc: 99.21%, Val Loss: 0.0599, Val Acc: 98.29%\n",
      "Epoch [387/500], Train Loss: 0.0233, Train Acc: 99.21%, Val Loss: 0.0493, Val Acc: 98.69%\n",
      "Epoch [388/500], Train Loss: 0.0228, Train Acc: 99.23%, Val Loss: 0.1641, Val Acc: 95.32%\n",
      "Epoch [389/500], Train Loss: 0.0226, Train Acc: 99.25%, Val Loss: 0.0425, Val Acc: 98.80%\n",
      "Epoch [390/500], Train Loss: 0.0225, Train Acc: 99.22%, Val Loss: 0.0540, Val Acc: 98.49%\n",
      "Epoch [391/500], Train Loss: 0.0215, Train Acc: 99.26%, Val Loss: 0.0419, Val Acc: 98.88%\n",
      "Epoch [392/500], Train Loss: 0.0218, Train Acc: 99.25%, Val Loss: 0.1722, Val Acc: 95.37%\n",
      "Epoch [393/500], Train Loss: 0.0219, Train Acc: 99.24%, Val Loss: 0.0447, Val Acc: 98.69%\n",
      "Epoch [394/500], Train Loss: 0.0224, Train Acc: 99.22%, Val Loss: 0.0562, Val Acc: 98.33%\n",
      "Epoch [395/500], Train Loss: 0.0215, Train Acc: 99.29%, Val Loss: 0.0409, Val Acc: 98.86%\n",
      "Epoch [396/500], Train Loss: 0.0228, Train Acc: 99.25%, Val Loss: 0.0439, Val Acc: 98.75%\n",
      "Epoch [397/500], Train Loss: 0.0209, Train Acc: 99.28%, Val Loss: 0.0448, Val Acc: 98.77%\n",
      "Epoch [398/500], Train Loss: 0.0223, Train Acc: 99.21%, Val Loss: 0.0459, Val Acc: 98.78%\n",
      "Epoch [399/500], Train Loss: 0.0216, Train Acc: 99.29%, Val Loss: 0.0580, Val Acc: 98.44%\n",
      "Epoch [400/500], Train Loss: 0.0217, Train Acc: 99.28%, Val Loss: 0.0719, Val Acc: 97.97%\n",
      "Epoch [401/500], Train Loss: 0.0218, Train Acc: 99.26%, Val Loss: 0.0387, Val Acc: 98.88%\n",
      "Epoch [402/500], Train Loss: 0.0213, Train Acc: 99.30%, Val Loss: 0.0434, Val Acc: 98.84%\n",
      "Epoch [403/500], Train Loss: 0.0218, Train Acc: 99.24%, Val Loss: 0.0975, Val Acc: 97.32%\n",
      "Epoch [404/500], Train Loss: 0.0207, Train Acc: 99.28%, Val Loss: 0.0603, Val Acc: 98.33%\n",
      "Epoch [405/500], Train Loss: 0.0214, Train Acc: 99.28%, Val Loss: 0.0448, Val Acc: 98.81%\n",
      "Epoch [406/500], Train Loss: 0.0227, Train Acc: 99.21%, Val Loss: 0.0455, Val Acc: 98.83%\n",
      "Epoch [407/500], Train Loss: 0.0221, Train Acc: 99.27%, Val Loss: 0.0398, Val Acc: 98.83%\n",
      "Epoch [408/500], Train Loss: 0.0213, Train Acc: 99.29%, Val Loss: 0.0470, Val Acc: 98.78%\n",
      "Epoch [409/500], Train Loss: 0.0225, Train Acc: 99.20%, Val Loss: 0.2920, Val Acc: 91.17%\n",
      "Epoch [410/500], Train Loss: 0.0210, Train Acc: 99.28%, Val Loss: 0.2649, Val Acc: 92.33%\n",
      "Epoch [411/500], Train Loss: 0.0215, Train Acc: 99.26%, Val Loss: 0.0468, Val Acc: 98.81%\n",
      "Epoch [412/500], Train Loss: 0.0222, Train Acc: 99.20%, Val Loss: 0.0449, Val Acc: 98.75%\n",
      "Epoch [413/500], Train Loss: 0.0224, Train Acc: 99.23%, Val Loss: 0.0530, Val Acc: 98.53%\n",
      "Epoch [414/500], Train Loss: 0.0204, Train Acc: 99.32%, Val Loss: 0.2437, Val Acc: 93.89%\n",
      "Epoch [415/500], Train Loss: 0.0217, Train Acc: 99.25%, Val Loss: 0.0578, Val Acc: 98.46%\n",
      "Epoch [416/500], Train Loss: 0.0212, Train Acc: 99.25%, Val Loss: 0.0927, Val Acc: 97.24%\n",
      "Epoch [417/500], Train Loss: 0.0197, Train Acc: 99.32%, Val Loss: 0.0430, Val Acc: 98.78%\n",
      "Epoch [418/500], Train Loss: 0.0207, Train Acc: 99.31%, Val Loss: 0.0599, Val Acc: 98.33%\n",
      "Epoch [419/500], Train Loss: 0.0215, Train Acc: 99.28%, Val Loss: 0.0500, Val Acc: 98.63%\n",
      "Epoch [420/500], Train Loss: 0.0213, Train Acc: 99.28%, Val Loss: 0.0561, Val Acc: 98.62%\n",
      "Epoch [421/500], Train Loss: 0.0193, Train Acc: 99.37%, Val Loss: 0.1061, Val Acc: 96.91%\n",
      "Epoch [422/500], Train Loss: 0.0200, Train Acc: 99.33%, Val Loss: 0.0509, Val Acc: 98.61%\n",
      "Epoch [423/500], Train Loss: 0.0208, Train Acc: 99.31%, Val Loss: 0.7871, Val Acc: 77.77%\n",
      "Epoch [424/500], Train Loss: 0.0211, Train Acc: 99.28%, Val Loss: 0.3118, Val Acc: 91.81%\n",
      "Epoch [425/500], Train Loss: 0.0221, Train Acc: 99.21%, Val Loss: 0.0923, Val Acc: 97.61%\n",
      "Epoch [426/500], Train Loss: 0.0215, Train Acc: 99.24%, Val Loss: 0.0491, Val Acc: 98.68%\n",
      "Epoch [427/500], Train Loss: 0.0212, Train Acc: 99.29%, Val Loss: 0.0506, Val Acc: 98.65%\n",
      "Epoch [428/500], Train Loss: 0.0198, Train Acc: 99.33%, Val Loss: 0.2433, Val Acc: 92.42%\n",
      "Epoch [429/500], Train Loss: 0.0217, Train Acc: 99.23%, Val Loss: 0.0486, Val Acc: 98.71%\n",
      "Epoch [430/500], Train Loss: 0.0194, Train Acc: 99.31%, Val Loss: 0.0485, Val Acc: 98.76%\n",
      "Epoch [431/500], Train Loss: 0.0215, Train Acc: 99.24%, Val Loss: 0.0438, Val Acc: 98.77%\n",
      "Epoch [432/500], Train Loss: 0.0193, Train Acc: 99.34%, Val Loss: 0.0467, Val Acc: 98.79%\n",
      "Epoch [433/500], Train Loss: 0.0191, Train Acc: 99.36%, Val Loss: 0.0451, Val Acc: 98.78%\n",
      "Epoch [434/500], Train Loss: 0.0198, Train Acc: 99.32%, Val Loss: 0.0440, Val Acc: 98.85%\n",
      "Epoch [435/500], Train Loss: 0.0197, Train Acc: 99.28%, Val Loss: 0.0391, Val Acc: 98.92%\n",
      "Epoch [436/500], Train Loss: 0.0206, Train Acc: 99.33%, Val Loss: 0.0863, Val Acc: 97.51%\n",
      "Epoch [437/500], Train Loss: 0.0205, Train Acc: 99.28%, Val Loss: 0.0494, Val Acc: 98.67%\n",
      "Epoch [438/500], Train Loss: 0.0227, Train Acc: 99.24%, Val Loss: 0.0468, Val Acc: 98.77%\n",
      "Epoch [439/500], Train Loss: 0.0207, Train Acc: 99.29%, Val Loss: 0.0504, Val Acc: 98.72%\n",
      "Epoch [440/500], Train Loss: 0.0199, Train Acc: 99.31%, Val Loss: 0.0467, Val Acc: 98.72%\n",
      "Epoch [441/500], Train Loss: 0.0213, Train Acc: 99.29%, Val Loss: 0.0683, Val Acc: 98.00%\n",
      "Epoch [442/500], Train Loss: 0.0187, Train Acc: 99.32%, Val Loss: 0.0447, Val Acc: 98.74%\n",
      "Epoch [443/500], Train Loss: 0.0193, Train Acc: 99.33%, Val Loss: 0.0452, Val Acc: 98.87%\n",
      "Epoch [444/500], Train Loss: 0.0189, Train Acc: 99.36%, Val Loss: 0.0453, Val Acc: 98.73%\n",
      "Epoch [445/500], Train Loss: 0.0192, Train Acc: 99.33%, Val Loss: 0.0502, Val Acc: 98.68%\n",
      "Epoch [446/500], Train Loss: 0.0202, Train Acc: 99.31%, Val Loss: 0.0396, Val Acc: 98.92%\n",
      "Epoch [447/500], Train Loss: 0.0188, Train Acc: 99.35%, Val Loss: 0.0445, Val Acc: 98.87%\n",
      "Epoch [448/500], Train Loss: 0.0193, Train Acc: 99.36%, Val Loss: 0.1538, Val Acc: 95.77%\n",
      "Epoch [449/500], Train Loss: 0.0200, Train Acc: 99.30%, Val Loss: 0.0466, Val Acc: 98.85%\n",
      "Epoch [450/500], Train Loss: 0.0210, Train Acc: 99.30%, Val Loss: 0.0586, Val Acc: 98.41%\n",
      "Epoch [451/500], Train Loss: 0.0185, Train Acc: 99.36%, Val Loss: 0.0437, Val Acc: 98.88%\n",
      "Epoch [452/500], Train Loss: 0.0205, Train Acc: 99.35%, Val Loss: 0.0600, Val Acc: 98.53%\n",
      "Epoch [453/500], Train Loss: 0.0190, Train Acc: 99.34%, Val Loss: 0.0455, Val Acc: 98.80%\n",
      "Epoch [454/500], Train Loss: 0.0183, Train Acc: 99.35%, Val Loss: 0.0505, Val Acc: 98.67%\n",
      "Epoch [455/500], Train Loss: 0.0196, Train Acc: 99.34%, Val Loss: 0.0462, Val Acc: 98.72%\n",
      "Epoch [456/500], Train Loss: 0.0199, Train Acc: 99.29%, Val Loss: 0.0708, Val Acc: 98.17%\n",
      "Epoch [457/500], Train Loss: 0.0209, Train Acc: 99.29%, Val Loss: 0.0624, Val Acc: 98.27%\n",
      "Epoch [458/500], Train Loss: 0.0194, Train Acc: 99.33%, Val Loss: 0.0496, Val Acc: 98.74%\n",
      "Epoch [459/500], Train Loss: 0.0198, Train Acc: 99.34%, Val Loss: 0.0400, Val Acc: 98.79%\n",
      "Epoch [460/500], Train Loss: 0.0190, Train Acc: 99.33%, Val Loss: 0.0491, Val Acc: 98.70%\n",
      "Epoch [461/500], Train Loss: 0.0196, Train Acc: 99.32%, Val Loss: 0.0470, Val Acc: 98.83%\n",
      "Epoch [462/500], Train Loss: 0.0208, Train Acc: 99.26%, Val Loss: 0.0444, Val Acc: 98.86%\n",
      "Epoch [463/500], Train Loss: 0.0195, Train Acc: 99.29%, Val Loss: 0.0583, Val Acc: 98.47%\n",
      "Epoch [464/500], Train Loss: 0.0181, Train Acc: 99.38%, Val Loss: 0.0600, Val Acc: 98.45%\n",
      "Epoch [465/500], Train Loss: 0.0207, Train Acc: 99.29%, Val Loss: 0.0470, Val Acc: 98.80%\n",
      "Epoch [466/500], Train Loss: 0.0199, Train Acc: 99.35%, Val Loss: 0.0530, Val Acc: 98.59%\n",
      "Epoch [467/500], Train Loss: 0.0196, Train Acc: 99.35%, Val Loss: 0.5159, Val Acc: 85.11%\n",
      "Epoch [468/500], Train Loss: 0.0193, Train Acc: 99.30%, Val Loss: 0.0523, Val Acc: 98.56%\n",
      "Epoch [469/500], Train Loss: 0.0177, Train Acc: 99.38%, Val Loss: 0.0453, Val Acc: 98.77%\n",
      "Epoch [470/500], Train Loss: 0.0185, Train Acc: 99.35%, Val Loss: 0.0879, Val Acc: 97.87%\n",
      "Epoch [471/500], Train Loss: 0.0187, Train Acc: 99.38%, Val Loss: 0.0471, Val Acc: 98.78%\n",
      "Epoch [472/500], Train Loss: 0.0187, Train Acc: 99.37%, Val Loss: 0.0445, Val Acc: 98.80%\n",
      "Epoch [473/500], Train Loss: 0.0197, Train Acc: 99.34%, Val Loss: 0.0497, Val Acc: 98.57%\n",
      "Epoch [474/500], Train Loss: 0.0175, Train Acc: 99.38%, Val Loss: 0.0529, Val Acc: 98.66%\n",
      "Epoch [475/500], Train Loss: 0.0188, Train Acc: 99.35%, Val Loss: 0.0477, Val Acc: 98.76%\n",
      "Epoch [476/500], Train Loss: 0.0188, Train Acc: 99.37%, Val Loss: 0.1468, Val Acc: 95.71%\n",
      "Epoch [477/500], Train Loss: 0.0178, Train Acc: 99.37%, Val Loss: 0.0444, Val Acc: 98.79%\n",
      "Epoch [478/500], Train Loss: 0.0191, Train Acc: 99.33%, Val Loss: 0.1823, Val Acc: 94.85%\n",
      "Epoch [479/500], Train Loss: 0.0189, Train Acc: 99.36%, Val Loss: 0.4124, Val Acc: 89.36%\n",
      "Epoch [480/500], Train Loss: 0.0179, Train Acc: 99.38%, Val Loss: 0.0670, Val Acc: 98.17%\n",
      "Epoch [481/500], Train Loss: 0.0186, Train Acc: 99.33%, Val Loss: 0.0486, Val Acc: 98.79%\n",
      "Epoch [482/500], Train Loss: 0.0187, Train Acc: 99.35%, Val Loss: 0.0564, Val Acc: 98.56%\n",
      "Epoch [483/500], Train Loss: 0.0198, Train Acc: 99.29%, Val Loss: 0.0497, Val Acc: 98.73%\n",
      "Epoch [484/500], Train Loss: 0.0185, Train Acc: 99.34%, Val Loss: 0.0485, Val Acc: 98.78%\n",
      "Epoch [485/500], Train Loss: 0.0178, Train Acc: 99.39%, Val Loss: 0.0551, Val Acc: 98.41%\n",
      "Epoch [486/500], Train Loss: 0.0193, Train Acc: 99.35%, Val Loss: 0.0547, Val Acc: 98.53%\n",
      "Epoch [487/500], Train Loss: 0.0182, Train Acc: 99.41%, Val Loss: 0.0427, Val Acc: 98.88%\n",
      "Epoch [488/500], Train Loss: 0.0184, Train Acc: 99.37%, Val Loss: 0.0688, Val Acc: 98.03%\n",
      "Epoch [489/500], Train Loss: 0.0186, Train Acc: 99.39%, Val Loss: 0.0479, Val Acc: 98.73%\n",
      "Epoch [490/500], Train Loss: 0.0184, Train Acc: 99.38%, Val Loss: 0.0464, Val Acc: 98.78%\n",
      "Epoch [491/500], Train Loss: 0.0179, Train Acc: 99.38%, Val Loss: 0.0497, Val Acc: 98.59%\n",
      "Epoch [492/500], Train Loss: 0.0176, Train Acc: 99.41%, Val Loss: 0.0492, Val Acc: 98.72%\n",
      "Epoch [493/500], Train Loss: 0.0182, Train Acc: 99.34%, Val Loss: 0.0395, Val Acc: 98.97%\n",
      "Epoch [494/500], Train Loss: 0.0199, Train Acc: 99.33%, Val Loss: 0.2034, Val Acc: 94.11%\n",
      "Epoch [495/500], Train Loss: 0.0192, Train Acc: 99.36%, Val Loss: 0.6052, Val Acc: 82.90%\n",
      "Epoch [496/500], Train Loss: 0.0176, Train Acc: 99.40%, Val Loss: 0.0442, Val Acc: 98.88%\n",
      "Epoch [497/500], Train Loss: 0.0193, Train Acc: 99.33%, Val Loss: 0.0419, Val Acc: 98.88%\n",
      "Epoch [498/500], Train Loss: 0.0185, Train Acc: 99.41%, Val Loss: 0.0440, Val Acc: 98.81%\n",
      "Epoch [499/500], Train Loss: 0.0183, Train Acc: 99.39%, Val Loss: 0.0490, Val Acc: 98.74%\n",
      "Epoch [500/500], Train Loss: 0.0186, Train Acc: 99.36%, Val Loss: 0.2511, Val Acc: 92.56%\n",
      "Test Accuracy of the best model on the test images: 98.73%\n"
     ]
    }
   ],
   "source": [
    "model = cfg_6c3f_1w1a_minst.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "num_epochs = 500\n",
    "best_val_acc = 0.0\n",
    "\n",
    "model_save_path = model_dir + \"/cfg_6c3f_1w1a_minst.pth\"\n",
    "best_model_save_path = model_dir + \"/best_cfg_6c3f_1w1a_minst.pth\"\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    if hasattr(model, 'clip_weights'):\n",
    "        model.clip_weights(-1.0, 1.0)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    \n",
    "    # \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_model_save_path)\n",
    "\n",
    "# 6. \n",
    "model.load_state_dict(torch.load(best_model_save_path))\n",
    "test_acc = test(model, test_loader, device)\n",
    "print(f'Test Accuracy of the best model on the test images: {test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
